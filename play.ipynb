{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of NETWORK(\n",
       "  (embedding): Embedding(1000, 128)\n",
       "  (rnn): LSTM(128, 256, batch_first=True)\n",
       "  (rnn1): LSTM(256, 300, batch_first=True)\n",
       "  (rnn2): LSTM(300, 200, batch_first=True)\n",
       "  (rnn3): LSTM(200, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=17, bias=True)\n",
       "  (batch_norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import RnnNetworkModel\n",
    "import constant as co\n",
    "import common_resources as cr\n",
    "torch,device=cr.get_torch_data()\n",
    "model = RnnNetworkModel.NETWORK(input_dim=co.INPUT_DIM,\n",
    "            embedding_dim=co.EMBEDDING_DIM,\n",
    "            hidden_dim=co.HIDDEN_DIM,\n",
    "            output_dim=co.NUM_CLASSES\n",
    ")\n",
    "\n",
    "model = model.to(device=device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.9) \n",
    "clip_value = 1.0\n",
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_source=cr.get_data_source()\n",
    "\n",
    "#get tokenizer object\n",
    "token=cr.get_tokenizer_object(data_source)\n",
    "\n",
    "#get encoded input and out put value\n",
    "input_date,out_put_date=cr.tokenize_input_out_put_field(token=token,data=data_source)\n",
    "\n",
    "# split into batch\n",
    "batch_date=cr.batch_spliter(input_data_sources=input_date,out_data_sources=out_put_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# from sequence data\n",
    "import pandas as pd\n",
    "data_source=cr.get_data_source()\n",
    "\n",
    "#get tokenizer object\n",
    "token=cr.get_tokenizer_object(data_source)\n",
    "seq_data=pd.read_csv('C:\\\\Toothless\\\\data\\\\sequence.csv')\n",
    "seq_data\n",
    "\n",
    "input_seq_data,_=cr.tokenize_input_out_put_field(token,seq_data)\n",
    "def dataConverter(data):\n",
    "     return list(map(int, data.split(\"$\")))\n",
    "out_seq_data=[dataConverter(i) for i in seq_data.get(\"status\").values.tolist()]\n",
    "batch=cr.batch_spliter(input_seq_data,out_seq_data,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "# batch=batch*100\n",
    "# random.shuffle(batch)\n",
    "batch_date=[*batch_date,*batch]\n",
    "\n",
    "# random.shuffle(batch_date)\n",
    "# random.shuffle(batch_date)\n",
    "len(batch_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hidden_state(data):\n",
    "        if data is not None:\n",
    "            return (data[0].to(device),data[1].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(logits, output)       \n\u001b[0;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 19\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip_value)\n\u001b[0;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Toothless\\.venv\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Toothless\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "NUM_EPOCHS=3\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    count=0\n",
    "    hidden_state=None\n",
    "    for i in batch :\n",
    "        input,output,sequence=i.get(\"input\"),i.get(\"output\"),i.get(\"sequence\")\n",
    "        # print(hidden_state)\n",
    "        if hidden_state is not None:\n",
    "                hidden_state = [(h.to(device), c.to(device)) for h, c in hidden_state if h is not None and c is not None]\n",
    "                model.load_hidden_state(hidden_state)\n",
    "        logits = model(input,sequence)\n",
    "        # print(output.shape)\n",
    "        # print(logits.shape)\n",
    "        loss = criterion(logits, output)       \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n",
    "        optimizer.step()\n",
    "        # model.reset_hidden_state()\n",
    "        # hidden_state=model.save_hidden_state()\n",
    "        # if i.get(\"isSeq\"):\n",
    "        #      hidden_state=model.save_hidden_state()\n",
    "        # else:\n",
    "        #     model.reset_hidden_state()\n",
    "\n",
    "        hidden_state=model.save_hidden_state() \n",
    "        # model.reset_hidden_state()       \n",
    "        print (f'Epoch: {epoch+1:03d}/{co.NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {count:03d}/{len(batch_date):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "        count+=1\n",
    "    # model.reset_hidden_state()\n",
    "    scheduler.step()\n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[127]['output']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=cr.batch_spliter(input_seq_data,out_seq_data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#savemodel\n",
    "torch.save(model.state_dict(), 'model_ini_up_v1_2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of NETWORK(\n",
       "  (embedding): Embedding(1000, 128)\n",
       "  (rnn): LSTM(128, 256, batch_first=True)\n",
       "  (rnn1): LSTM(256, 300, batch_first=True)\n",
       "  (rnn2): LSTM(300, 200, batch_first=True)\n",
       "  (rnn3): LSTM(200, 256, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=17, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import RnnNetworkModel\n",
    "import constant as co\n",
    "import common_resources as cr\n",
    "torch,device=cr.get_torch_data()\n",
    "model1 = RnnNetworkModel.NETWORK(input_dim=co.INPUT_DIM,\n",
    "            embedding_dim=co.EMBEDDING_DIM,\n",
    "            hidden_dim=co.HIDDEN_DIM,\n",
    "            output_dim=co.NUM_CLASSES\n",
    ")\n",
    "\n",
    "model1 = model1.to(device=device)\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.005)\n",
    "model1.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model\n",
    "model1.load_state_dict(torch.load('C:\\\\Toothless\\\\model_ini_up.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/001 | Batch 000/137 | Loss: 264.9913\n",
      "Epoch: 001/001 | Batch 001/137 | Loss: 180.2625\n",
      "Epoch: 001/001 | Batch 002/137 | Loss: 113.5064\n",
      "Epoch: 001/001 | Batch 003/137 | Loss: 70.9202\n",
      "Epoch: 001/001 | Batch 004/137 | Loss: 68.6320\n",
      "Epoch: 001/001 | Batch 005/137 | Loss: 56.2609\n",
      "Epoch: 001/001 | Batch 006/137 | Loss: 49.9164\n",
      "Epoch: 001/001 | Batch 007/137 | Loss: 47.1169\n",
      "Epoch: 001/001 | Batch 008/137 | Loss: 39.6987\n",
      "Epoch: 001/001 | Batch 009/137 | Loss: 42.9246\n",
      "Epoch: 001/001 | Batch 010/137 | Loss: 39.5247\n",
      "Epoch: 001/001 | Batch 011/137 | Loss: 39.7180\n",
      "Epoch: 001/001 | Batch 012/137 | Loss: 36.7565\n",
      "Epoch: 001/001 | Batch 013/137 | Loss: 39.6622\n",
      "Epoch: 001/001 | Batch 014/137 | Loss: 38.0344\n",
      "Epoch: 001/001 | Batch 015/137 | Loss: 35.6606\n",
      "Epoch: 001/001 | Batch 016/137 | Loss: 39.5027\n",
      "Epoch: 001/001 | Batch 017/137 | Loss: 37.5552\n",
      "Epoch: 001/001 | Batch 018/137 | Loss: 38.4259\n",
      "Epoch: 001/001 | Batch 019/137 | Loss: 38.4520\n",
      "Epoch: 001/001 | Batch 020/137 | Loss: 35.7009\n",
      "Epoch: 001/001 | Batch 021/137 | Loss: 36.9022\n",
      "Epoch: 001/001 | Batch 022/137 | Loss: 37.3563\n",
      "Epoch: 001/001 | Batch 023/137 | Loss: 20.9418\n",
      "Epoch: 002/001 | Batch 000/137 | Loss: 38.5364\n",
      "Epoch: 002/001 | Batch 001/137 | Loss: 39.1251\n",
      "Epoch: 002/001 | Batch 002/137 | Loss: 38.8867\n",
      "Epoch: 002/001 | Batch 003/137 | Loss: 35.5726\n",
      "Epoch: 002/001 | Batch 004/137 | Loss: 40.6865\n",
      "Epoch: 002/001 | Batch 005/137 | Loss: 37.7348\n",
      "Epoch: 002/001 | Batch 006/137 | Loss: 38.9119\n",
      "Epoch: 002/001 | Batch 007/137 | Loss: 40.4979\n",
      "Epoch: 002/001 | Batch 008/137 | Loss: 36.1368\n",
      "Epoch: 002/001 | Batch 009/137 | Loss: 39.2997\n",
      "Epoch: 002/001 | Batch 010/137 | Loss: 36.8590\n",
      "Epoch: 002/001 | Batch 011/137 | Loss: 37.3082\n",
      "Epoch: 002/001 | Batch 012/137 | Loss: 35.2227\n",
      "Epoch: 002/001 | Batch 013/137 | Loss: 38.6954\n",
      "Epoch: 002/001 | Batch 014/137 | Loss: 37.4865\n",
      "Epoch: 002/001 | Batch 015/137 | Loss: 35.4325\n",
      "Epoch: 002/001 | Batch 016/137 | Loss: 38.8642\n",
      "Epoch: 002/001 | Batch 017/137 | Loss: 37.1511\n",
      "Epoch: 002/001 | Batch 018/137 | Loss: 38.1987\n",
      "Epoch: 002/001 | Batch 019/137 | Loss: 38.2788\n",
      "Epoch: 002/001 | Batch 020/137 | Loss: 35.5319\n",
      "Epoch: 002/001 | Batch 021/137 | Loss: 36.3971\n",
      "Epoch: 002/001 | Batch 022/137 | Loss: 37.1453\n",
      "Epoch: 002/001 | Batch 023/137 | Loss: 20.8632\n",
      "Epoch: 003/001 | Batch 000/137 | Loss: 38.2944\n",
      "Epoch: 003/001 | Batch 001/137 | Loss: 38.9569\n",
      "Epoch: 003/001 | Batch 002/137 | Loss: 38.7288\n",
      "Epoch: 003/001 | Batch 003/137 | Loss: 35.4863\n",
      "Epoch: 003/001 | Batch 004/137 | Loss: 40.5444\n",
      "Epoch: 003/001 | Batch 005/137 | Loss: 37.6267\n",
      "Epoch: 003/001 | Batch 006/137 | Loss: 38.7193\n",
      "Epoch: 003/001 | Batch 007/137 | Loss: 40.3704\n",
      "Epoch: 003/001 | Batch 008/137 | Loss: 35.9752\n",
      "Epoch: 003/001 | Batch 009/137 | Loss: 39.2902\n",
      "Epoch: 003/001 | Batch 010/137 | Loss: 36.7210\n",
      "Epoch: 003/001 | Batch 011/137 | Loss: 37.1910\n",
      "Epoch: 003/001 | Batch 012/137 | Loss: 35.1950\n",
      "Epoch: 003/001 | Batch 013/137 | Loss: 38.6648\n",
      "Epoch: 003/001 | Batch 014/137 | Loss: 37.4304\n",
      "Epoch: 003/001 | Batch 015/137 | Loss: 35.4619\n",
      "Epoch: 003/001 | Batch 016/137 | Loss: 38.8305\n",
      "Epoch: 003/001 | Batch 017/137 | Loss: 37.1063\n",
      "Epoch: 003/001 | Batch 018/137 | Loss: 38.1811\n",
      "Epoch: 003/001 | Batch 019/137 | Loss: 38.2273\n",
      "Epoch: 003/001 | Batch 020/137 | Loss: 35.4686\n",
      "Epoch: 003/001 | Batch 021/137 | Loss: 36.3726\n",
      "Epoch: 003/001 | Batch 022/137 | Loss: 37.1307\n",
      "Epoch: 003/001 | Batch 023/137 | Loss: 20.7176\n",
      "Epoch: 004/001 | Batch 000/137 | Loss: 38.1977\n",
      "Epoch: 004/001 | Batch 001/137 | Loss: 38.9384\n",
      "Epoch: 004/001 | Batch 002/137 | Loss: 38.7006\n",
      "Epoch: 004/001 | Batch 003/137 | Loss: 35.4067\n",
      "Epoch: 004/001 | Batch 004/137 | Loss: 40.5372\n",
      "Epoch: 004/001 | Batch 005/137 | Loss: 37.7055\n",
      "Epoch: 004/001 | Batch 006/137 | Loss: 38.6856\n",
      "Epoch: 004/001 | Batch 007/137 | Loss: 40.3744\n",
      "Epoch: 004/001 | Batch 008/137 | Loss: 35.9566\n",
      "Epoch: 004/001 | Batch 009/137 | Loss: 39.3340\n",
      "Epoch: 004/001 | Batch 010/137 | Loss: 36.6668\n",
      "Epoch: 004/001 | Batch 011/137 | Loss: 37.1540\n",
      "Epoch: 004/001 | Batch 012/137 | Loss: 35.1783\n",
      "Epoch: 004/001 | Batch 013/137 | Loss: 38.6236\n",
      "Epoch: 004/001 | Batch 014/137 | Loss: 37.3786\n",
      "Epoch: 004/001 | Batch 015/137 | Loss: 35.4191\n",
      "Epoch: 004/001 | Batch 016/137 | Loss: 38.8389\n",
      "Epoch: 004/001 | Batch 017/137 | Loss: 37.0776\n",
      "Epoch: 004/001 | Batch 018/137 | Loss: 38.1591\n",
      "Epoch: 004/001 | Batch 019/137 | Loss: 38.1902\n",
      "Epoch: 004/001 | Batch 020/137 | Loss: 35.4597\n",
      "Epoch: 004/001 | Batch 021/137 | Loss: 36.3691\n",
      "Epoch: 004/001 | Batch 022/137 | Loss: 37.1106\n",
      "Epoch: 004/001 | Batch 023/137 | Loss: 20.6754\n",
      "Epoch: 005/001 | Batch 000/137 | Loss: 38.1472\n",
      "Epoch: 005/001 | Batch 001/137 | Loss: 38.9561\n",
      "Epoch: 005/001 | Batch 002/137 | Loss: 38.6829\n",
      "Epoch: 005/001 | Batch 003/137 | Loss: 35.3661\n",
      "Epoch: 005/001 | Batch 004/137 | Loss: 40.5183\n",
      "Epoch: 005/001 | Batch 005/137 | Loss: 37.7170\n",
      "Epoch: 005/001 | Batch 006/137 | Loss: 38.6628\n",
      "Epoch: 005/001 | Batch 007/137 | Loss: 40.3477\n",
      "Epoch: 005/001 | Batch 008/137 | Loss: 35.9305\n",
      "Epoch: 005/001 | Batch 009/137 | Loss: 39.3215\n",
      "Epoch: 005/001 | Batch 010/137 | Loss: 36.6463\n",
      "Epoch: 005/001 | Batch 011/137 | Loss: 37.1394\n",
      "Epoch: 005/001 | Batch 012/137 | Loss: 35.1609\n",
      "Epoch: 005/001 | Batch 013/137 | Loss: 38.6048\n",
      "Epoch: 005/001 | Batch 014/137 | Loss: 37.3481\n",
      "Epoch: 005/001 | Batch 015/137 | Loss: 35.4044\n",
      "Epoch: 005/001 | Batch 016/137 | Loss: 38.8393\n",
      "Epoch: 005/001 | Batch 017/137 | Loss: 37.0453\n",
      "Epoch: 005/001 | Batch 018/137 | Loss: 38.1365\n",
      "Epoch: 005/001 | Batch 019/137 | Loss: 38.1703\n",
      "Epoch: 005/001 | Batch 020/137 | Loss: 35.4329\n",
      "Epoch: 005/001 | Batch 021/137 | Loss: 36.3369\n",
      "Epoch: 005/001 | Batch 022/137 | Loss: 37.0885\n",
      "Epoch: 005/001 | Batch 023/137 | Loss: 20.6069\n",
      "Epoch: 006/001 | Batch 000/137 | Loss: 38.1169\n",
      "Epoch: 006/001 | Batch 001/137 | Loss: 38.9390\n",
      "Epoch: 006/001 | Batch 002/137 | Loss: 38.6522\n",
      "Epoch: 006/001 | Batch 003/137 | Loss: 35.3242\n",
      "Epoch: 006/001 | Batch 004/137 | Loss: 40.5106\n",
      "Epoch: 006/001 | Batch 005/137 | Loss: 37.7454\n",
      "Epoch: 006/001 | Batch 006/137 | Loss: 38.6593\n",
      "Epoch: 006/001 | Batch 007/137 | Loss: 40.3284\n",
      "Epoch: 006/001 | Batch 008/137 | Loss: 35.9079\n",
      "Epoch: 006/001 | Batch 009/137 | Loss: 39.3005\n",
      "Epoch: 006/001 | Batch 010/137 | Loss: 36.6466\n",
      "Epoch: 006/001 | Batch 011/137 | Loss: 37.1374\n",
      "Epoch: 006/001 | Batch 012/137 | Loss: 35.1435\n",
      "Epoch: 006/001 | Batch 013/137 | Loss: 38.5987\n",
      "Epoch: 006/001 | Batch 014/137 | Loss: 37.3453\n",
      "Epoch: 006/001 | Batch 015/137 | Loss: 35.3927\n",
      "Epoch: 006/001 | Batch 016/137 | Loss: 38.8255\n",
      "Epoch: 006/001 | Batch 017/137 | Loss: 37.0377\n",
      "Epoch: 006/001 | Batch 018/137 | Loss: 38.1188\n",
      "Epoch: 006/001 | Batch 019/137 | Loss: 38.1741\n",
      "Epoch: 006/001 | Batch 020/137 | Loss: 35.4119\n",
      "Epoch: 006/001 | Batch 021/137 | Loss: 36.3233\n",
      "Epoch: 006/001 | Batch 022/137 | Loss: 37.0654\n",
      "Epoch: 006/001 | Batch 023/137 | Loss: 20.5453\n",
      "Epoch: 007/001 | Batch 000/137 | Loss: 38.1064\n",
      "Epoch: 007/001 | Batch 001/137 | Loss: 38.9006\n",
      "Epoch: 007/001 | Batch 002/137 | Loss: 38.6175\n",
      "Epoch: 007/001 | Batch 003/137 | Loss: 35.2928\n",
      "Epoch: 007/001 | Batch 004/137 | Loss: 40.4975\n",
      "Epoch: 007/001 | Batch 005/137 | Loss: 37.7527\n",
      "Epoch: 007/001 | Batch 006/137 | Loss: 38.6494\n",
      "Epoch: 007/001 | Batch 007/137 | Loss: 40.3126\n",
      "Epoch: 007/001 | Batch 008/137 | Loss: 35.8879\n",
      "Epoch: 007/001 | Batch 009/137 | Loss: 39.2879\n",
      "Epoch: 007/001 | Batch 010/137 | Loss: 36.6320\n",
      "Epoch: 007/001 | Batch 011/137 | Loss: 37.1292\n",
      "Epoch: 007/001 | Batch 012/137 | Loss: 35.1168\n",
      "Epoch: 007/001 | Batch 013/137 | Loss: 38.5949\n",
      "Epoch: 007/001 | Batch 014/137 | Loss: 37.3415\n",
      "Epoch: 007/001 | Batch 015/137 | Loss: 35.3725\n",
      "Epoch: 007/001 | Batch 016/137 | Loss: 38.8031\n",
      "Epoch: 007/001 | Batch 017/137 | Loss: 37.0359\n",
      "Epoch: 007/001 | Batch 018/137 | Loss: 38.1082\n",
      "Epoch: 007/001 | Batch 019/137 | Loss: 38.1647\n",
      "Epoch: 007/001 | Batch 020/137 | Loss: 35.3990\n",
      "Epoch: 007/001 | Batch 021/137 | Loss: 36.3156\n",
      "Epoch: 007/001 | Batch 022/137 | Loss: 37.0439\n",
      "Epoch: 007/001 | Batch 023/137 | Loss: 20.4956\n",
      "Epoch: 008/001 | Batch 000/137 | Loss: 38.1062\n",
      "Epoch: 008/001 | Batch 001/137 | Loss: 38.8798\n",
      "Epoch: 008/001 | Batch 002/137 | Loss: 38.5956\n",
      "Epoch: 008/001 | Batch 003/137 | Loss: 35.2808\n",
      "Epoch: 008/001 | Batch 004/137 | Loss: 40.4743\n",
      "Epoch: 008/001 | Batch 005/137 | Loss: 37.7233\n",
      "Epoch: 008/001 | Batch 006/137 | Loss: 38.6016\n",
      "Epoch: 008/001 | Batch 007/137 | Loss: 40.2988\n",
      "Epoch: 008/001 | Batch 008/137 | Loss: 35.8770\n",
      "Epoch: 008/001 | Batch 009/137 | Loss: 39.2811\n",
      "Epoch: 008/001 | Batch 010/137 | Loss: 36.6009\n",
      "Epoch: 008/001 | Batch 011/137 | Loss: 37.1057\n",
      "Epoch: 008/001 | Batch 012/137 | Loss: 35.0950\n",
      "Epoch: 008/001 | Batch 013/137 | Loss: 38.5657\n",
      "Epoch: 008/001 | Batch 014/137 | Loss: 37.3307\n",
      "Epoch: 008/001 | Batch 015/137 | Loss: 35.3667\n",
      "Epoch: 008/001 | Batch 016/137 | Loss: 38.7888\n",
      "Epoch: 008/001 | Batch 017/137 | Loss: 37.0218\n",
      "Epoch: 008/001 | Batch 018/137 | Loss: 38.0947\n",
      "Epoch: 008/001 | Batch 019/137 | Loss: 38.1413\n",
      "Epoch: 008/001 | Batch 020/137 | Loss: 35.3823\n",
      "Epoch: 008/001 | Batch 021/137 | Loss: 36.2865\n",
      "Epoch: 008/001 | Batch 022/137 | Loss: 37.0253\n",
      "Epoch: 008/001 | Batch 023/137 | Loss: 20.4145\n",
      "Epoch: 009/001 | Batch 000/137 | Loss: 38.0795\n",
      "Epoch: 009/001 | Batch 001/137 | Loss: 38.8502\n",
      "Epoch: 009/001 | Batch 002/137 | Loss: 38.5772\n",
      "Epoch: 009/001 | Batch 003/137 | Loss: 35.2678\n",
      "Epoch: 009/001 | Batch 004/137 | Loss: 40.4555\n",
      "Epoch: 009/001 | Batch 005/137 | Loss: 37.6886\n",
      "Epoch: 009/001 | Batch 006/137 | Loss: 38.5834\n",
      "Epoch: 009/001 | Batch 007/137 | Loss: 40.2859\n",
      "Epoch: 009/001 | Batch 008/137 | Loss: 35.8581\n",
      "Epoch: 009/001 | Batch 009/137 | Loss: 39.2729\n",
      "Epoch: 009/001 | Batch 010/137 | Loss: 36.5646\n",
      "Epoch: 009/001 | Batch 011/137 | Loss: 37.0815\n",
      "Epoch: 009/001 | Batch 012/137 | Loss: 35.0704\n",
      "Epoch: 009/001 | Batch 013/137 | Loss: 38.5453\n",
      "Epoch: 009/001 | Batch 014/137 | Loss: 37.3055\n",
      "Epoch: 009/001 | Batch 015/137 | Loss: 35.3447\n",
      "Epoch: 009/001 | Batch 016/137 | Loss: 38.7774\n",
      "Epoch: 009/001 | Batch 017/137 | Loss: 37.0004\n",
      "Epoch: 009/001 | Batch 018/137 | Loss: 38.0771\n",
      "Epoch: 009/001 | Batch 019/137 | Loss: 38.1161\n",
      "Epoch: 009/001 | Batch 020/137 | Loss: 35.3595\n",
      "Epoch: 009/001 | Batch 021/137 | Loss: 36.2752\n",
      "Epoch: 009/001 | Batch 022/137 | Loss: 36.9898\n",
      "Epoch: 009/001 | Batch 023/137 | Loss: 20.3257\n",
      "Epoch: 010/001 | Batch 000/137 | Loss: 38.0527\n",
      "Epoch: 010/001 | Batch 001/137 | Loss: 38.8359\n",
      "Epoch: 010/001 | Batch 002/137 | Loss: 38.5557\n",
      "Epoch: 010/001 | Batch 003/137 | Loss: 35.2259\n",
      "Epoch: 010/001 | Batch 004/137 | Loss: 40.4292\n",
      "Epoch: 010/001 | Batch 005/137 | Loss: 37.6277\n",
      "Epoch: 010/001 | Batch 006/137 | Loss: 38.5565\n",
      "Epoch: 010/001 | Batch 007/137 | Loss: 40.2497\n",
      "Epoch: 010/001 | Batch 008/137 | Loss: 35.8358\n",
      "Epoch: 010/001 | Batch 009/137 | Loss: 39.2647\n",
      "Epoch: 010/001 | Batch 010/137 | Loss: 36.5566\n",
      "Epoch: 010/001 | Batch 011/137 | Loss: 37.0591\n",
      "Epoch: 010/001 | Batch 012/137 | Loss: 35.0481\n",
      "Epoch: 010/001 | Batch 013/137 | Loss: 38.5244\n",
      "Epoch: 010/001 | Batch 014/137 | Loss: 37.2814\n",
      "Epoch: 010/001 | Batch 015/137 | Loss: 35.3295\n",
      "Epoch: 010/001 | Batch 016/137 | Loss: 38.7769\n",
      "Epoch: 010/001 | Batch 017/137 | Loss: 36.9753\n",
      "Epoch: 010/001 | Batch 018/137 | Loss: 38.0680\n",
      "Epoch: 010/001 | Batch 019/137 | Loss: 38.0856\n",
      "Epoch: 010/001 | Batch 020/137 | Loss: 35.3246\n",
      "Epoch: 010/001 | Batch 021/137 | Loss: 36.2507\n",
      "Epoch: 010/001 | Batch 022/137 | Loss: 36.9710\n",
      "Epoch: 010/001 | Batch 023/137 | Loss: 20.2007\n",
      "Epoch: 011/001 | Batch 000/137 | Loss: 38.0414\n",
      "Epoch: 011/001 | Batch 001/137 | Loss: 38.8138\n",
      "Epoch: 011/001 | Batch 002/137 | Loss: 38.5468\n",
      "Epoch: 011/001 | Batch 003/137 | Loss: 35.2138\n",
      "Epoch: 011/001 | Batch 004/137 | Loss: 40.4134\n",
      "Epoch: 011/001 | Batch 005/137 | Loss: 37.6326\n",
      "Epoch: 011/001 | Batch 006/137 | Loss: 38.5603\n",
      "Epoch: 011/001 | Batch 007/137 | Loss: 40.2391\n",
      "Epoch: 011/001 | Batch 008/137 | Loss: 35.8056\n",
      "Epoch: 011/001 | Batch 009/137 | Loss: 39.2193\n",
      "Epoch: 011/001 | Batch 010/137 | Loss: 36.5507\n",
      "Epoch: 011/001 | Batch 011/137 | Loss: 37.0707\n",
      "Epoch: 011/001 | Batch 012/137 | Loss: 35.0238\n",
      "Epoch: 011/001 | Batch 013/137 | Loss: 38.4970\n",
      "Epoch: 011/001 | Batch 014/137 | Loss: 37.2750\n",
      "Epoch: 011/001 | Batch 015/137 | Loss: 35.3449\n",
      "Epoch: 011/001 | Batch 016/137 | Loss: 38.7521\n",
      "Epoch: 011/001 | Batch 017/137 | Loss: 36.9501\n",
      "Epoch: 011/001 | Batch 018/137 | Loss: 38.0613\n",
      "Epoch: 011/001 | Batch 019/137 | Loss: 38.0832\n",
      "Epoch: 011/001 | Batch 020/137 | Loss: 35.3066\n",
      "Epoch: 011/001 | Batch 021/137 | Loss: 36.2359\n",
      "Epoch: 011/001 | Batch 022/137 | Loss: 36.9923\n",
      "Epoch: 011/001 | Batch 023/137 | Loss: 20.0956\n",
      "Epoch: 012/001 | Batch 000/137 | Loss: 38.0376\n",
      "Epoch: 012/001 | Batch 001/137 | Loss: 38.8097\n",
      "Epoch: 012/001 | Batch 002/137 | Loss: 38.5186\n",
      "Epoch: 012/001 | Batch 003/137 | Loss: 35.1943\n",
      "Epoch: 012/001 | Batch 004/137 | Loss: 40.3807\n",
      "Epoch: 012/001 | Batch 005/137 | Loss: 37.5836\n",
      "Epoch: 012/001 | Batch 006/137 | Loss: 38.5418\n",
      "Epoch: 012/001 | Batch 007/137 | Loss: 40.2356\n",
      "Epoch: 012/001 | Batch 008/137 | Loss: 35.7927\n",
      "Epoch: 012/001 | Batch 009/137 | Loss: 39.2175\n",
      "Epoch: 012/001 | Batch 010/137 | Loss: 36.5121\n",
      "Epoch: 012/001 | Batch 011/137 | Loss: 37.0566\n",
      "Epoch: 012/001 | Batch 012/137 | Loss: 34.9918\n",
      "Epoch: 012/001 | Batch 013/137 | Loss: 38.4833\n",
      "Epoch: 012/001 | Batch 014/137 | Loss: 37.2493\n",
      "Epoch: 012/001 | Batch 015/137 | Loss: 35.3207\n",
      "Epoch: 012/001 | Batch 016/137 | Loss: 38.7521\n",
      "Epoch: 012/001 | Batch 017/137 | Loss: 36.9433\n",
      "Epoch: 012/001 | Batch 018/137 | Loss: 38.0548\n",
      "Epoch: 012/001 | Batch 019/137 | Loss: 38.0445\n",
      "Epoch: 012/001 | Batch 020/137 | Loss: 35.2878\n",
      "Epoch: 012/001 | Batch 021/137 | Loss: 36.2261\n",
      "Epoch: 012/001 | Batch 022/137 | Loss: 36.9607\n",
      "Epoch: 012/001 | Batch 023/137 | Loss: 20.0968\n",
      "Epoch: 013/001 | Batch 000/137 | Loss: 38.0045\n",
      "Epoch: 013/001 | Batch 001/137 | Loss: 38.8047\n",
      "Epoch: 013/001 | Batch 002/137 | Loss: 38.5223\n",
      "Epoch: 013/001 | Batch 003/137 | Loss: 35.1990\n",
      "Epoch: 013/001 | Batch 004/137 | Loss: 40.4122\n",
      "Epoch: 013/001 | Batch 005/137 | Loss: 37.5958\n",
      "Epoch: 013/001 | Batch 006/137 | Loss: 38.5249\n",
      "Epoch: 013/001 | Batch 007/137 | Loss: 40.2214\n",
      "Epoch: 013/001 | Batch 008/137 | Loss: 35.7772\n",
      "Epoch: 013/001 | Batch 009/137 | Loss: 39.2072\n",
      "Epoch: 013/001 | Batch 010/137 | Loss: 36.5314\n",
      "Epoch: 013/001 | Batch 011/137 | Loss: 37.0722\n",
      "Epoch: 013/001 | Batch 012/137 | Loss: 35.0021\n",
      "Epoch: 013/001 | Batch 013/137 | Loss: 38.4913\n",
      "Epoch: 013/001 | Batch 014/137 | Loss: 37.2679\n",
      "Epoch: 013/001 | Batch 015/137 | Loss: 35.3555\n",
      "Epoch: 013/001 | Batch 016/137 | Loss: 38.7441\n",
      "Epoch: 013/001 | Batch 017/137 | Loss: 36.9373\n",
      "Epoch: 013/001 | Batch 018/137 | Loss: 38.0727\n",
      "Epoch: 013/001 | Batch 019/137 | Loss: 38.0956\n",
      "Epoch: 013/001 | Batch 020/137 | Loss: 35.2972\n",
      "Epoch: 013/001 | Batch 021/137 | Loss: 36.2464\n",
      "Epoch: 013/001 | Batch 022/137 | Loss: 36.9541\n",
      "Epoch: 013/001 | Batch 023/137 | Loss: 19.9907\n",
      "Epoch: 014/001 | Batch 000/137 | Loss: 38.0393\n",
      "Epoch: 014/001 | Batch 001/137 | Loss: 38.8071\n",
      "Epoch: 014/001 | Batch 002/137 | Loss: 38.5207\n",
      "Epoch: 014/001 | Batch 003/137 | Loss: 35.1838\n",
      "Epoch: 014/001 | Batch 004/137 | Loss: 40.3748\n",
      "Epoch: 014/001 | Batch 005/137 | Loss: 37.5556\n",
      "Epoch: 014/001 | Batch 006/137 | Loss: 38.5160\n",
      "Epoch: 014/001 | Batch 007/137 | Loss: 40.2129\n",
      "Epoch: 014/001 | Batch 008/137 | Loss: 35.7743\n",
      "Epoch: 014/001 | Batch 009/137 | Loss: 39.2117\n",
      "Epoch: 014/001 | Batch 010/137 | Loss: 36.5256\n",
      "Epoch: 014/001 | Batch 011/137 | Loss: 37.0372\n",
      "Epoch: 014/001 | Batch 012/137 | Loss: 34.9863\n",
      "Epoch: 014/001 | Batch 013/137 | Loss: 38.4748\n",
      "Epoch: 014/001 | Batch 014/137 | Loss: 37.2622\n",
      "Epoch: 014/001 | Batch 015/137 | Loss: 35.3409\n",
      "Epoch: 014/001 | Batch 016/137 | Loss: 38.7448\n",
      "Epoch: 014/001 | Batch 017/137 | Loss: 36.9235\n",
      "Epoch: 014/001 | Batch 018/137 | Loss: 38.0294\n",
      "Epoch: 014/001 | Batch 019/137 | Loss: 38.0388\n",
      "Epoch: 014/001 | Batch 020/137 | Loss: 35.2692\n",
      "Epoch: 014/001 | Batch 021/137 | Loss: 36.2197\n",
      "Epoch: 014/001 | Batch 022/137 | Loss: 36.9435\n",
      "Epoch: 014/001 | Batch 023/137 | Loss: 19.9148\n",
      "Epoch: 015/001 | Batch 000/137 | Loss: 37.9895\n",
      "Epoch: 015/001 | Batch 001/137 | Loss: 38.7761\n",
      "Epoch: 015/001 | Batch 002/137 | Loss: 38.4761\n",
      "Epoch: 015/001 | Batch 003/137 | Loss: 35.1569\n",
      "Epoch: 015/001 | Batch 004/137 | Loss: 40.3870\n",
      "Epoch: 015/001 | Batch 005/137 | Loss: 37.5617\n",
      "Epoch: 015/001 | Batch 006/137 | Loss: 38.5099\n",
      "Epoch: 015/001 | Batch 007/137 | Loss: 40.1954\n",
      "Epoch: 015/001 | Batch 008/137 | Loss: 35.7549\n",
      "Epoch: 015/001 | Batch 009/137 | Loss: 39.1451\n",
      "Epoch: 015/001 | Batch 010/137 | Loss: 36.5348\n",
      "Epoch: 015/001 | Batch 011/137 | Loss: 37.0406\n",
      "Epoch: 015/001 | Batch 012/137 | Loss: 34.9492\n",
      "Epoch: 015/001 | Batch 013/137 | Loss: 38.4560\n",
      "Epoch: 015/001 | Batch 014/137 | Loss: 37.2330\n",
      "Epoch: 015/001 | Batch 015/137 | Loss: 35.3520\n",
      "Epoch: 015/001 | Batch 016/137 | Loss: 38.7413\n",
      "Epoch: 015/001 | Batch 017/137 | Loss: 36.9043\n",
      "Epoch: 015/001 | Batch 018/137 | Loss: 38.0383\n",
      "Epoch: 015/001 | Batch 019/137 | Loss: 38.0257\n",
      "Epoch: 015/001 | Batch 020/137 | Loss: 35.2463\n",
      "Epoch: 015/001 | Batch 021/137 | Loss: 36.1979\n",
      "Epoch: 015/001 | Batch 022/137 | Loss: 36.8859\n",
      "Epoch: 015/001 | Batch 023/137 | Loss: 19.8080\n",
      "Epoch: 016/001 | Batch 000/137 | Loss: 37.9790\n",
      "Epoch: 016/001 | Batch 001/137 | Loss: 38.7546\n",
      "Epoch: 016/001 | Batch 002/137 | Loss: 38.5010\n",
      "Epoch: 016/001 | Batch 003/137 | Loss: 35.1452\n",
      "Epoch: 016/001 | Batch 004/137 | Loss: 40.3408\n",
      "Epoch: 016/001 | Batch 005/137 | Loss: 37.5263\n",
      "Epoch: 016/001 | Batch 006/137 | Loss: 38.5138\n",
      "Epoch: 016/001 | Batch 007/137 | Loss: 40.1911\n",
      "Epoch: 016/001 | Batch 008/137 | Loss: 35.7517\n",
      "Epoch: 016/001 | Batch 009/137 | Loss: 39.1415\n",
      "Epoch: 016/001 | Batch 010/137 | Loss: 36.5047\n",
      "Epoch: 016/001 | Batch 011/137 | Loss: 37.0441\n",
      "Epoch: 016/001 | Batch 012/137 | Loss: 34.9377\n",
      "Epoch: 016/001 | Batch 013/137 | Loss: 38.4683\n",
      "Epoch: 016/001 | Batch 014/137 | Loss: 37.2311\n",
      "Epoch: 016/001 | Batch 015/137 | Loss: 35.2986\n",
      "Epoch: 016/001 | Batch 016/137 | Loss: 38.6851\n",
      "Epoch: 016/001 | Batch 017/137 | Loss: 36.8800\n",
      "Epoch: 016/001 | Batch 018/137 | Loss: 38.0320\n",
      "Epoch: 016/001 | Batch 019/137 | Loss: 38.0036\n",
      "Epoch: 016/001 | Batch 020/137 | Loss: 35.2637\n",
      "Epoch: 016/001 | Batch 021/137 | Loss: 36.1764\n",
      "Epoch: 016/001 | Batch 022/137 | Loss: 36.9010\n",
      "Epoch: 016/001 | Batch 023/137 | Loss: 19.7290\n",
      "Epoch: 017/001 | Batch 000/137 | Loss: 37.9859\n",
      "Epoch: 017/001 | Batch 001/137 | Loss: 38.7309\n",
      "Epoch: 017/001 | Batch 002/137 | Loss: 38.4924\n",
      "Epoch: 017/001 | Batch 003/137 | Loss: 35.1545\n",
      "Epoch: 017/001 | Batch 004/137 | Loss: 40.3176\n",
      "Epoch: 017/001 | Batch 005/137 | Loss: 37.5088\n",
      "Epoch: 017/001 | Batch 006/137 | Loss: 38.5277\n",
      "Epoch: 017/001 | Batch 007/137 | Loss: 40.1952\n",
      "Epoch: 017/001 | Batch 008/137 | Loss: 35.7441\n",
      "Epoch: 017/001 | Batch 009/137 | Loss: 39.1623\n",
      "Epoch: 017/001 | Batch 010/137 | Loss: 36.5145\n",
      "Epoch: 017/001 | Batch 011/137 | Loss: 37.0539\n",
      "Epoch: 017/001 | Batch 012/137 | Loss: 34.9138\n",
      "Epoch: 017/001 | Batch 013/137 | Loss: 38.4407\n",
      "Epoch: 017/001 | Batch 014/137 | Loss: 37.2469\n",
      "Epoch: 017/001 | Batch 015/137 | Loss: 35.2649\n",
      "Epoch: 017/001 | Batch 016/137 | Loss: 38.6831\n",
      "Epoch: 017/001 | Batch 017/137 | Loss: 36.8715\n",
      "Epoch: 017/001 | Batch 018/137 | Loss: 38.0331\n",
      "Epoch: 017/001 | Batch 019/137 | Loss: 37.9785\n",
      "Epoch: 017/001 | Batch 020/137 | Loss: 35.2284\n",
      "Epoch: 017/001 | Batch 021/137 | Loss: 36.1722\n",
      "Epoch: 017/001 | Batch 022/137 | Loss: 36.9013\n",
      "Epoch: 017/001 | Batch 023/137 | Loss: 19.6089\n",
      "Epoch: 018/001 | Batch 000/137 | Loss: 37.9372\n",
      "Epoch: 018/001 | Batch 001/137 | Loss: 38.7164\n",
      "Epoch: 018/001 | Batch 002/137 | Loss: 38.5064\n",
      "Epoch: 018/001 | Batch 003/137 | Loss: 35.1500\n",
      "Epoch: 018/001 | Batch 004/137 | Loss: 40.3135\n",
      "Epoch: 018/001 | Batch 005/137 | Loss: 37.4424\n",
      "Epoch: 018/001 | Batch 006/137 | Loss: 38.4861\n",
      "Epoch: 018/001 | Batch 007/137 | Loss: 40.1462\n",
      "Epoch: 018/001 | Batch 008/137 | Loss: 35.6944\n",
      "Epoch: 018/001 | Batch 009/137 | Loss: 39.1310\n",
      "Epoch: 018/001 | Batch 010/137 | Loss: 36.4269\n",
      "Epoch: 018/001 | Batch 011/137 | Loss: 37.0024\n",
      "Epoch: 018/001 | Batch 012/137 | Loss: 34.8872\n",
      "Epoch: 018/001 | Batch 013/137 | Loss: 38.4073\n",
      "Epoch: 018/001 | Batch 014/137 | Loss: 37.2078\n",
      "Epoch: 018/001 | Batch 015/137 | Loss: 35.2390\n",
      "Epoch: 018/001 | Batch 016/137 | Loss: 38.6325\n",
      "Epoch: 018/001 | Batch 017/137 | Loss: 36.8627\n",
      "Epoch: 018/001 | Batch 018/137 | Loss: 38.0037\n",
      "Epoch: 018/001 | Batch 019/137 | Loss: 37.9571\n",
      "Epoch: 018/001 | Batch 020/137 | Loss: 35.1942\n",
      "Epoch: 018/001 | Batch 021/137 | Loss: 36.1622\n",
      "Epoch: 018/001 | Batch 022/137 | Loss: 36.8714\n",
      "Epoch: 018/001 | Batch 023/137 | Loss: 19.4937\n",
      "Epoch: 019/001 | Batch 000/137 | Loss: 37.9060\n",
      "Epoch: 019/001 | Batch 001/137 | Loss: 38.7557\n",
      "Epoch: 019/001 | Batch 002/137 | Loss: 38.5134\n",
      "Epoch: 019/001 | Batch 003/137 | Loss: 35.1141\n",
      "Epoch: 019/001 | Batch 004/137 | Loss: 40.2844\n",
      "Epoch: 019/001 | Batch 005/137 | Loss: 37.4200\n",
      "Epoch: 019/001 | Batch 006/137 | Loss: 38.4591\n",
      "Epoch: 019/001 | Batch 007/137 | Loss: 40.1376\n",
      "Epoch: 019/001 | Batch 008/137 | Loss: 35.6769\n",
      "Epoch: 019/001 | Batch 009/137 | Loss: 39.0939\n",
      "Epoch: 019/001 | Batch 010/137 | Loss: 36.4321\n",
      "Epoch: 019/001 | Batch 011/137 | Loss: 36.9729\n",
      "Epoch: 019/001 | Batch 012/137 | Loss: 34.8693\n",
      "Epoch: 019/001 | Batch 013/137 | Loss: 38.4072\n",
      "Epoch: 019/001 | Batch 014/137 | Loss: 37.1929\n",
      "Epoch: 019/001 | Batch 015/137 | Loss: 35.2196\n",
      "Epoch: 019/001 | Batch 016/137 | Loss: 38.7013\n",
      "Epoch: 019/001 | Batch 017/137 | Loss: 36.8390\n",
      "Epoch: 019/001 | Batch 018/137 | Loss: 37.9892\n",
      "Epoch: 019/001 | Batch 019/137 | Loss: 37.9422\n",
      "Epoch: 019/001 | Batch 020/137 | Loss: 35.1717\n",
      "Epoch: 019/001 | Batch 021/137 | Loss: 36.1573\n",
      "Epoch: 019/001 | Batch 022/137 | Loss: 36.8241\n",
      "Epoch: 019/001 | Batch 023/137 | Loss: 19.4188\n",
      "Epoch: 020/001 | Batch 000/137 | Loss: 37.8911\n",
      "Epoch: 020/001 | Batch 001/137 | Loss: 38.7062\n",
      "Epoch: 020/001 | Batch 002/137 | Loss: 38.4966\n",
      "Epoch: 020/001 | Batch 003/137 | Loss: 35.0538\n",
      "Epoch: 020/001 | Batch 004/137 | Loss: 40.2828\n",
      "Epoch: 020/001 | Batch 005/137 | Loss: 37.3852\n",
      "Epoch: 020/001 | Batch 006/137 | Loss: 38.4412\n",
      "Epoch: 020/001 | Batch 007/137 | Loss: 40.1404\n",
      "Epoch: 020/001 | Batch 008/137 | Loss: 35.6673\n",
      "Epoch: 020/001 | Batch 009/137 | Loss: 39.0586\n",
      "Epoch: 020/001 | Batch 010/137 | Loss: 36.4225\n",
      "Epoch: 020/001 | Batch 011/137 | Loss: 36.9791\n",
      "Epoch: 020/001 | Batch 012/137 | Loss: 34.8358\n",
      "Epoch: 020/001 | Batch 013/137 | Loss: 38.3866\n",
      "Epoch: 020/001 | Batch 014/137 | Loss: 37.1732\n",
      "Epoch: 020/001 | Batch 015/137 | Loss: 35.2170\n",
      "Epoch: 020/001 | Batch 016/137 | Loss: 38.6353\n",
      "Epoch: 020/001 | Batch 017/137 | Loss: 36.8243\n",
      "Epoch: 020/001 | Batch 018/137 | Loss: 37.9659\n",
      "Epoch: 020/001 | Batch 019/137 | Loss: 37.9108\n",
      "Epoch: 020/001 | Batch 020/137 | Loss: 35.1850\n",
      "Epoch: 020/001 | Batch 021/137 | Loss: 36.1143\n",
      "Epoch: 020/001 | Batch 022/137 | Loss: 36.7902\n",
      "Epoch: 020/001 | Batch 023/137 | Loss: 19.3336\n",
      "Epoch: 021/001 | Batch 000/137 | Loss: 37.8662\n",
      "Epoch: 021/001 | Batch 001/137 | Loss: 38.6849\n",
      "Epoch: 021/001 | Batch 002/137 | Loss: 38.4403\n",
      "Epoch: 021/001 | Batch 003/137 | Loss: 35.0466\n",
      "Epoch: 021/001 | Batch 004/137 | Loss: 40.2920\n",
      "Epoch: 021/001 | Batch 005/137 | Loss: 37.3852\n",
      "Epoch: 021/001 | Batch 006/137 | Loss: 38.4550\n",
      "Epoch: 021/001 | Batch 007/137 | Loss: 40.1291\n",
      "Epoch: 021/001 | Batch 008/137 | Loss: 35.6471\n",
      "Epoch: 021/001 | Batch 009/137 | Loss: 39.0218\n",
      "Epoch: 021/001 | Batch 010/137 | Loss: 36.4346\n",
      "Epoch: 021/001 | Batch 011/137 | Loss: 37.0261\n",
      "Epoch: 021/001 | Batch 012/137 | Loss: 34.8249\n",
      "Epoch: 021/001 | Batch 013/137 | Loss: 38.3923\n",
      "Epoch: 021/001 | Batch 014/137 | Loss: 37.1984\n",
      "Epoch: 021/001 | Batch 015/137 | Loss: 35.1780\n",
      "Epoch: 021/001 | Batch 016/137 | Loss: 38.6339\n",
      "Epoch: 021/001 | Batch 017/137 | Loss: 36.8262\n",
      "Epoch: 021/001 | Batch 018/137 | Loss: 37.9874\n",
      "Epoch: 021/001 | Batch 019/137 | Loss: 37.8930\n",
      "Epoch: 021/001 | Batch 020/137 | Loss: 35.1507\n",
      "Epoch: 021/001 | Batch 021/137 | Loss: 36.1362\n",
      "Epoch: 021/001 | Batch 022/137 | Loss: 36.7892\n",
      "Epoch: 021/001 | Batch 023/137 | Loss: 19.4902\n",
      "Epoch: 022/001 | Batch 000/137 | Loss: 37.8612\n",
      "Epoch: 022/001 | Batch 001/137 | Loss: 38.7101\n",
      "Epoch: 022/001 | Batch 002/137 | Loss: 38.4541\n",
      "Epoch: 022/001 | Batch 003/137 | Loss: 35.0382\n",
      "Epoch: 022/001 | Batch 004/137 | Loss: 40.2692\n",
      "Epoch: 022/001 | Batch 005/137 | Loss: 37.4225\n",
      "Epoch: 022/001 | Batch 006/137 | Loss: 38.5090\n",
      "Epoch: 022/001 | Batch 007/137 | Loss: 40.1781\n",
      "Epoch: 022/001 | Batch 008/137 | Loss: 35.6421\n",
      "Epoch: 022/001 | Batch 009/137 | Loss: 39.0564\n",
      "Epoch: 022/001 | Batch 010/137 | Loss: 36.4041\n",
      "Epoch: 022/001 | Batch 011/137 | Loss: 36.9792\n",
      "Epoch: 022/001 | Batch 012/137 | Loss: 34.8556\n",
      "Epoch: 022/001 | Batch 013/137 | Loss: 38.3993\n",
      "Epoch: 022/001 | Batch 014/137 | Loss: 37.2089\n",
      "Epoch: 022/001 | Batch 015/137 | Loss: 35.2122\n",
      "Epoch: 022/001 | Batch 016/137 | Loss: 38.6139\n",
      "Epoch: 022/001 | Batch 017/137 | Loss: 36.8510\n",
      "Epoch: 022/001 | Batch 018/137 | Loss: 38.0060\n",
      "Epoch: 022/001 | Batch 019/137 | Loss: 37.9432\n",
      "Epoch: 022/001 | Batch 020/137 | Loss: 35.1596\n",
      "Epoch: 022/001 | Batch 021/137 | Loss: 36.1006\n",
      "Epoch: 022/001 | Batch 022/137 | Loss: 36.7833\n",
      "Epoch: 022/001 | Batch 023/137 | Loss: 19.4274\n",
      "Epoch: 023/001 | Batch 000/137 | Loss: 37.8572\n",
      "Epoch: 023/001 | Batch 001/137 | Loss: 38.7170\n",
      "Epoch: 023/001 | Batch 002/137 | Loss: 38.4168\n",
      "Epoch: 023/001 | Batch 003/137 | Loss: 35.0461\n",
      "Epoch: 023/001 | Batch 004/137 | Loss: 40.3083\n",
      "Epoch: 023/001 | Batch 005/137 | Loss: 37.3688\n",
      "Epoch: 023/001 | Batch 006/137 | Loss: 38.4258\n",
      "Epoch: 023/001 | Batch 007/137 | Loss: 40.1531\n",
      "Epoch: 023/001 | Batch 008/137 | Loss: 35.6799\n",
      "Epoch: 023/001 | Batch 009/137 | Loss: 39.0625\n",
      "Epoch: 023/001 | Batch 010/137 | Loss: 36.4408\n",
      "Epoch: 023/001 | Batch 011/137 | Loss: 36.9970\n",
      "Epoch: 023/001 | Batch 012/137 | Loss: 34.7977\n",
      "Epoch: 023/001 | Batch 013/137 | Loss: 38.3983\n",
      "Epoch: 023/001 | Batch 014/137 | Loss: 37.2139\n",
      "Epoch: 023/001 | Batch 015/137 | Loss: 35.3054\n",
      "Epoch: 023/001 | Batch 016/137 | Loss: 38.6702\n",
      "Epoch: 023/001 | Batch 017/137 | Loss: 36.8023\n",
      "Epoch: 023/001 | Batch 018/137 | Loss: 37.9686\n",
      "Epoch: 023/001 | Batch 019/137 | Loss: 37.9281\n",
      "Epoch: 023/001 | Batch 020/137 | Loss: 35.1292\n",
      "Epoch: 023/001 | Batch 021/137 | Loss: 36.1177\n",
      "Epoch: 023/001 | Batch 022/137 | Loss: 36.8085\n",
      "Epoch: 023/001 | Batch 023/137 | Loss: 19.1041\n",
      "Epoch: 024/001 | Batch 000/137 | Loss: 37.8430\n",
      "Epoch: 024/001 | Batch 001/137 | Loss: 38.6822\n",
      "Epoch: 024/001 | Batch 002/137 | Loss: 38.4661\n",
      "Epoch: 024/001 | Batch 003/137 | Loss: 35.0947\n",
      "Epoch: 024/001 | Batch 004/137 | Loss: 40.2742\n",
      "Epoch: 024/001 | Batch 005/137 | Loss: 37.3814\n",
      "Epoch: 024/001 | Batch 006/137 | Loss: 38.4589\n",
      "Epoch: 024/001 | Batch 007/137 | Loss: 40.1172\n",
      "Epoch: 024/001 | Batch 008/137 | Loss: 35.6542\n",
      "Epoch: 024/001 | Batch 009/137 | Loss: 39.1002\n",
      "Epoch: 024/001 | Batch 010/137 | Loss: 36.3807\n",
      "Epoch: 024/001 | Batch 011/137 | Loss: 36.9895\n",
      "Epoch: 024/001 | Batch 012/137 | Loss: 34.8349\n",
      "Epoch: 024/001 | Batch 013/137 | Loss: 38.4157\n",
      "Epoch: 024/001 | Batch 014/137 | Loss: 37.2243\n",
      "Epoch: 024/001 | Batch 015/137 | Loss: 35.2441\n",
      "Epoch: 024/001 | Batch 016/137 | Loss: 38.6686\n",
      "Epoch: 024/001 | Batch 017/137 | Loss: 36.8072\n",
      "Epoch: 024/001 | Batch 018/137 | Loss: 37.9650\n",
      "Epoch: 024/001 | Batch 019/137 | Loss: 37.9271\n",
      "Epoch: 024/001 | Batch 020/137 | Loss: 35.1429\n",
      "Epoch: 024/001 | Batch 021/137 | Loss: 36.1745\n",
      "Epoch: 024/001 | Batch 022/137 | Loss: 36.8392\n",
      "Epoch: 024/001 | Batch 023/137 | Loss: 18.9510\n",
      "Epoch: 025/001 | Batch 000/137 | Loss: 37.8453\n",
      "Epoch: 025/001 | Batch 001/137 | Loss: 38.7446\n",
      "Epoch: 025/001 | Batch 002/137 | Loss: 38.4677\n",
      "Epoch: 025/001 | Batch 003/137 | Loss: 35.0818\n",
      "Epoch: 025/001 | Batch 004/137 | Loss: 40.3289\n",
      "Epoch: 025/001 | Batch 005/137 | Loss: 37.4208\n",
      "Epoch: 025/001 | Batch 006/137 | Loss: 38.4644\n",
      "Epoch: 025/001 | Batch 007/137 | Loss: 40.1661\n",
      "Epoch: 025/001 | Batch 008/137 | Loss: 35.6669\n",
      "Epoch: 025/001 | Batch 009/137 | Loss: 39.0536\n",
      "Epoch: 025/001 | Batch 010/137 | Loss: 36.4295\n",
      "Epoch: 025/001 | Batch 011/137 | Loss: 37.0111\n",
      "Epoch: 025/001 | Batch 012/137 | Loss: 34.8493\n",
      "Epoch: 025/001 | Batch 013/137 | Loss: 38.3845\n",
      "Epoch: 025/001 | Batch 014/137 | Loss: 37.1894\n",
      "Epoch: 025/001 | Batch 015/137 | Loss: 35.2169\n",
      "Epoch: 025/001 | Batch 016/137 | Loss: 38.6544\n",
      "Epoch: 025/001 | Batch 017/137 | Loss: 36.8495\n",
      "Epoch: 025/001 | Batch 018/137 | Loss: 37.9674\n",
      "Epoch: 025/001 | Batch 019/137 | Loss: 37.9272\n",
      "Epoch: 025/001 | Batch 020/137 | Loss: 35.1676\n",
      "Epoch: 025/001 | Batch 021/137 | Loss: 36.1272\n",
      "Epoch: 025/001 | Batch 022/137 | Loss: 36.8132\n",
      "Epoch: 025/001 | Batch 023/137 | Loss: 18.8969\n",
      "Epoch: 026/001 | Batch 000/137 | Loss: 37.8273\n",
      "Epoch: 026/001 | Batch 001/137 | Loss: 38.7062\n",
      "Epoch: 026/001 | Batch 002/137 | Loss: 38.4956\n",
      "Epoch: 026/001 | Batch 003/137 | Loss: 35.0498\n",
      "Epoch: 026/001 | Batch 004/137 | Loss: 40.3004\n",
      "Epoch: 026/001 | Batch 005/137 | Loss: 37.4011\n",
      "Epoch: 026/001 | Batch 006/137 | Loss: 38.4597\n",
      "Epoch: 026/001 | Batch 007/137 | Loss: 40.1450\n",
      "Epoch: 026/001 | Batch 008/137 | Loss: 35.6524\n",
      "Epoch: 026/001 | Batch 009/137 | Loss: 39.0128\n",
      "Epoch: 026/001 | Batch 010/137 | Loss: 36.3838\n",
      "Epoch: 026/001 | Batch 011/137 | Loss: 37.0173\n",
      "Epoch: 026/001 | Batch 012/137 | Loss: 34.8409\n",
      "Epoch: 026/001 | Batch 013/137 | Loss: 38.3869\n",
      "Epoch: 026/001 | Batch 014/137 | Loss: 37.1801\n",
      "Epoch: 026/001 | Batch 015/137 | Loss: 35.2102\n",
      "Epoch: 026/001 | Batch 016/137 | Loss: 38.6231\n",
      "Epoch: 026/001 | Batch 017/137 | Loss: 36.7963\n",
      "Epoch: 026/001 | Batch 018/137 | Loss: 37.9578\n",
      "Epoch: 026/001 | Batch 019/137 | Loss: 37.9232\n",
      "Epoch: 026/001 | Batch 020/137 | Loss: 35.1265\n",
      "Epoch: 026/001 | Batch 021/137 | Loss: 36.1903\n",
      "Epoch: 026/001 | Batch 022/137 | Loss: 36.8207\n",
      "Epoch: 026/001 | Batch 023/137 | Loss: 18.8260\n",
      "Epoch: 027/001 | Batch 000/137 | Loss: 37.8301\n",
      "Epoch: 027/001 | Batch 001/137 | Loss: 38.7251\n",
      "Epoch: 027/001 | Batch 002/137 | Loss: 38.4675\n",
      "Epoch: 027/001 | Batch 003/137 | Loss: 35.0397\n",
      "Epoch: 027/001 | Batch 004/137 | Loss: 40.3103\n",
      "Epoch: 027/001 | Batch 005/137 | Loss: 37.3954\n",
      "Epoch: 027/001 | Batch 006/137 | Loss: 38.4850\n",
      "Epoch: 027/001 | Batch 007/137 | Loss: 40.1591\n",
      "Epoch: 027/001 | Batch 008/137 | Loss: 35.6682\n",
      "Epoch: 027/001 | Batch 009/137 | Loss: 39.0039\n",
      "Epoch: 027/001 | Batch 010/137 | Loss: 36.4974\n",
      "Epoch: 027/001 | Batch 011/137 | Loss: 37.0635\n",
      "Epoch: 027/001 | Batch 012/137 | Loss: 34.8768\n",
      "Epoch: 027/001 | Batch 013/137 | Loss: 38.4119\n",
      "Epoch: 027/001 | Batch 014/137 | Loss: 37.2446\n",
      "Epoch: 027/001 | Batch 015/137 | Loss: 35.2476\n",
      "Epoch: 027/001 | Batch 016/137 | Loss: 38.6821\n",
      "Epoch: 027/001 | Batch 017/137 | Loss: 36.8256\n",
      "Epoch: 027/001 | Batch 018/137 | Loss: 37.9975\n",
      "Epoch: 027/001 | Batch 019/137 | Loss: 37.9202\n",
      "Epoch: 027/001 | Batch 020/137 | Loss: 35.2230\n",
      "Epoch: 027/001 | Batch 021/137 | Loss: 36.1382\n",
      "Epoch: 027/001 | Batch 022/137 | Loss: 36.8598\n",
      "Epoch: 027/001 | Batch 023/137 | Loss: 18.7485\n",
      "Epoch: 028/001 | Batch 000/137 | Loss: 37.8685\n",
      "Epoch: 028/001 | Batch 001/137 | Loss: 38.7407\n",
      "Epoch: 028/001 | Batch 002/137 | Loss: 38.4997\n",
      "Epoch: 028/001 | Batch 003/137 | Loss: 35.0972\n",
      "Epoch: 028/001 | Batch 004/137 | Loss: 40.2777\n",
      "Epoch: 028/001 | Batch 005/137 | Loss: 37.3918\n",
      "Epoch: 028/001 | Batch 006/137 | Loss: 38.5389\n",
      "Epoch: 028/001 | Batch 007/137 | Loss: 40.1871\n",
      "Epoch: 028/001 | Batch 008/137 | Loss: 35.6848\n",
      "Epoch: 028/001 | Batch 009/137 | Loss: 39.0940\n",
      "Epoch: 028/001 | Batch 010/137 | Loss: 36.4791\n",
      "Epoch: 028/001 | Batch 011/137 | Loss: 37.0303\n",
      "Epoch: 028/001 | Batch 012/137 | Loss: 34.8430\n",
      "Epoch: 028/001 | Batch 013/137 | Loss: 38.4214\n",
      "Epoch: 028/001 | Batch 014/137 | Loss: 37.2866\n",
      "Epoch: 028/001 | Batch 015/137 | Loss: 35.3098\n",
      "Epoch: 028/001 | Batch 016/137 | Loss: 38.6635\n",
      "Epoch: 028/001 | Batch 017/137 | Loss: 36.8612\n",
      "Epoch: 028/001 | Batch 018/137 | Loss: 38.0294\n",
      "Epoch: 028/001 | Batch 019/137 | Loss: 37.9826\n",
      "Epoch: 028/001 | Batch 020/137 | Loss: 35.1917\n",
      "Epoch: 028/001 | Batch 021/137 | Loss: 36.1388\n",
      "Epoch: 028/001 | Batch 022/137 | Loss: 36.8685\n",
      "Epoch: 028/001 | Batch 023/137 | Loss: 18.8400\n",
      "Epoch: 029/001 | Batch 000/137 | Loss: 37.8527\n",
      "Epoch: 029/001 | Batch 001/137 | Loss: 38.7078\n",
      "Epoch: 029/001 | Batch 002/137 | Loss: 38.4902\n",
      "Epoch: 029/001 | Batch 003/137 | Loss: 35.0360\n",
      "Epoch: 029/001 | Batch 004/137 | Loss: 40.3274\n",
      "Epoch: 029/001 | Batch 005/137 | Loss: 37.4117\n",
      "Epoch: 029/001 | Batch 006/137 | Loss: 38.5488\n",
      "Epoch: 029/001 | Batch 007/137 | Loss: 40.2004\n",
      "Epoch: 029/001 | Batch 008/137 | Loss: 35.6803\n",
      "Epoch: 029/001 | Batch 009/137 | Loss: 39.0902\n",
      "Epoch: 029/001 | Batch 010/137 | Loss: 36.4967\n",
      "Epoch: 029/001 | Batch 011/137 | Loss: 37.0575\n",
      "Epoch: 029/001 | Batch 012/137 | Loss: 34.8466\n",
      "Epoch: 029/001 | Batch 013/137 | Loss: 38.4301\n",
      "Epoch: 029/001 | Batch 014/137 | Loss: 37.2299\n",
      "Epoch: 029/001 | Batch 015/137 | Loss: 35.2693\n",
      "Epoch: 029/001 | Batch 016/137 | Loss: 38.7102\n",
      "Epoch: 029/001 | Batch 017/137 | Loss: 36.8416\n",
      "Epoch: 029/001 | Batch 018/137 | Loss: 38.0365\n",
      "Epoch: 029/001 | Batch 019/137 | Loss: 37.9652\n",
      "Epoch: 029/001 | Batch 020/137 | Loss: 35.1584\n",
      "Epoch: 029/001 | Batch 021/137 | Loss: 36.1783\n",
      "Epoch: 029/001 | Batch 022/137 | Loss: 36.8464\n",
      "Epoch: 029/001 | Batch 023/137 | Loss: 18.7971\n",
      "Epoch: 030/001 | Batch 000/137 | Loss: 37.8436\n",
      "Epoch: 030/001 | Batch 001/137 | Loss: 38.6827\n",
      "Epoch: 030/001 | Batch 002/137 | Loss: 38.4996\n",
      "Epoch: 030/001 | Batch 003/137 | Loss: 35.0871\n",
      "Epoch: 030/001 | Batch 004/137 | Loss: 40.2885\n",
      "Epoch: 030/001 | Batch 005/137 | Loss: 37.4437\n",
      "Epoch: 030/001 | Batch 006/137 | Loss: 38.4957\n",
      "Epoch: 030/001 | Batch 007/137 | Loss: 40.2395\n",
      "Epoch: 030/001 | Batch 008/137 | Loss: 35.7382\n",
      "Epoch: 030/001 | Batch 009/137 | Loss: 39.0382\n",
      "Epoch: 030/001 | Batch 010/137 | Loss: 36.4538\n",
      "Epoch: 030/001 | Batch 011/137 | Loss: 37.0541\n",
      "Epoch: 030/001 | Batch 012/137 | Loss: 34.8450\n",
      "Epoch: 030/001 | Batch 013/137 | Loss: 38.4894\n",
      "Epoch: 030/001 | Batch 014/137 | Loss: 37.2855\n",
      "Epoch: 030/001 | Batch 015/137 | Loss: 35.2721\n",
      "Epoch: 030/001 | Batch 016/137 | Loss: 38.6218\n",
      "Epoch: 030/001 | Batch 017/137 | Loss: 36.8662\n",
      "Epoch: 030/001 | Batch 018/137 | Loss: 37.9885\n",
      "Epoch: 030/001 | Batch 019/137 | Loss: 37.9170\n",
      "Epoch: 030/001 | Batch 020/137 | Loss: 35.1626\n",
      "Epoch: 030/001 | Batch 021/137 | Loss: 36.2510\n",
      "Epoch: 030/001 | Batch 022/137 | Loss: 36.8298\n",
      "Epoch: 030/001 | Batch 023/137 | Loss: 18.6129\n",
      "Epoch: 031/001 | Batch 000/137 | Loss: 37.7887\n",
      "Epoch: 031/001 | Batch 001/137 | Loss: 38.7047\n",
      "Epoch: 031/001 | Batch 002/137 | Loss: 38.4472\n",
      "Epoch: 031/001 | Batch 003/137 | Loss: 35.0860\n",
      "Epoch: 031/001 | Batch 004/137 | Loss: 40.2237\n",
      "Epoch: 031/001 | Batch 005/137 | Loss: 37.4006\n",
      "Epoch: 031/001 | Batch 006/137 | Loss: 38.4889\n",
      "Epoch: 031/001 | Batch 007/137 | Loss: 40.1599\n",
      "Epoch: 031/001 | Batch 008/137 | Loss: 35.6475\n",
      "Epoch: 031/001 | Batch 009/137 | Loss: 39.0033\n",
      "Epoch: 031/001 | Batch 010/137 | Loss: 36.3999\n",
      "Epoch: 031/001 | Batch 011/137 | Loss: 36.9954\n",
      "Epoch: 031/001 | Batch 012/137 | Loss: 34.7778\n",
      "Epoch: 031/001 | Batch 013/137 | Loss: 38.3905\n",
      "Epoch: 031/001 | Batch 014/137 | Loss: 37.1768\n",
      "Epoch: 031/001 | Batch 015/137 | Loss: 35.2584\n",
      "Epoch: 031/001 | Batch 016/137 | Loss: 38.6907\n",
      "Epoch: 031/001 | Batch 017/137 | Loss: 36.8316\n",
      "Epoch: 031/001 | Batch 018/137 | Loss: 37.9515\n",
      "Epoch: 031/001 | Batch 019/137 | Loss: 37.9213\n",
      "Epoch: 031/001 | Batch 020/137 | Loss: 35.1103\n",
      "Epoch: 031/001 | Batch 021/137 | Loss: 36.1088\n",
      "Epoch: 031/001 | Batch 022/137 | Loss: 36.8172\n",
      "Epoch: 031/001 | Batch 023/137 | Loss: 18.4386\n",
      "Epoch: 032/001 | Batch 000/137 | Loss: 37.7938\n",
      "Epoch: 032/001 | Batch 001/137 | Loss: 38.7066\n",
      "Epoch: 032/001 | Batch 002/137 | Loss: 38.4839\n",
      "Epoch: 032/001 | Batch 003/137 | Loss: 35.1384\n",
      "Epoch: 032/001 | Batch 004/137 | Loss: 40.2598\n",
      "Epoch: 032/001 | Batch 005/137 | Loss: 37.3524\n",
      "Epoch: 032/001 | Batch 006/137 | Loss: 38.4662\n",
      "Epoch: 032/001 | Batch 007/137 | Loss: 40.1723\n",
      "Epoch: 032/001 | Batch 008/137 | Loss: 35.6783\n",
      "Epoch: 032/001 | Batch 009/137 | Loss: 39.0795\n",
      "Epoch: 032/001 | Batch 010/137 | Loss: 36.4214\n",
      "Epoch: 032/001 | Batch 011/137 | Loss: 37.0052\n",
      "Epoch: 032/001 | Batch 012/137 | Loss: 34.7939\n",
      "Epoch: 032/001 | Batch 013/137 | Loss: 38.3758\n",
      "Epoch: 032/001 | Batch 014/137 | Loss: 37.2465\n",
      "Epoch: 032/001 | Batch 015/137 | Loss: 35.2718\n",
      "Epoch: 032/001 | Batch 016/137 | Loss: 38.7179\n",
      "Epoch: 032/001 | Batch 017/137 | Loss: 36.8321\n",
      "Epoch: 032/001 | Batch 018/137 | Loss: 38.0263\n",
      "Epoch: 032/001 | Batch 019/137 | Loss: 37.9421\n",
      "Epoch: 032/001 | Batch 020/137 | Loss: 35.1482\n",
      "Epoch: 032/001 | Batch 021/137 | Loss: 36.1567\n",
      "Epoch: 032/001 | Batch 022/137 | Loss: 36.7930\n",
      "Epoch: 032/001 | Batch 023/137 | Loss: 18.4180\n",
      "Epoch: 033/001 | Batch 000/137 | Loss: 37.8163\n",
      "Epoch: 033/001 | Batch 001/137 | Loss: 38.6858\n",
      "Epoch: 033/001 | Batch 002/137 | Loss: 38.4662\n",
      "Epoch: 033/001 | Batch 003/137 | Loss: 35.1134\n",
      "Epoch: 033/001 | Batch 004/137 | Loss: 40.2658\n",
      "Epoch: 033/001 | Batch 005/137 | Loss: 37.4128\n",
      "Epoch: 033/001 | Batch 006/137 | Loss: 38.5695\n",
      "Epoch: 033/001 | Batch 007/137 | Loss: 40.2544\n",
      "Epoch: 033/001 | Batch 008/137 | Loss: 35.6535\n",
      "Epoch: 033/001 | Batch 009/137 | Loss: 39.0495\n",
      "Epoch: 033/001 | Batch 010/137 | Loss: 36.4170\n",
      "Epoch: 033/001 | Batch 011/137 | Loss: 37.0620\n",
      "Epoch: 033/001 | Batch 012/137 | Loss: 34.7907\n",
      "Epoch: 033/001 | Batch 013/137 | Loss: 38.4153\n",
      "Epoch: 033/001 | Batch 014/137 | Loss: 37.2669\n",
      "Epoch: 033/001 | Batch 015/137 | Loss: 35.2550\n",
      "Epoch: 033/001 | Batch 016/137 | Loss: 38.6657\n",
      "Epoch: 033/001 | Batch 017/137 | Loss: 36.8376\n",
      "Epoch: 033/001 | Batch 018/137 | Loss: 37.9644\n",
      "Epoch: 033/001 | Batch 019/137 | Loss: 37.9240\n",
      "Epoch: 033/001 | Batch 020/137 | Loss: 35.1573\n",
      "Epoch: 033/001 | Batch 021/137 | Loss: 36.1842\n",
      "Epoch: 033/001 | Batch 022/137 | Loss: 36.8271\n",
      "Epoch: 033/001 | Batch 023/137 | Loss: 18.2003\n",
      "Epoch: 034/001 | Batch 000/137 | Loss: 37.7708\n",
      "Epoch: 034/001 | Batch 001/137 | Loss: 38.6668\n",
      "Epoch: 034/001 | Batch 002/137 | Loss: 38.4945\n",
      "Epoch: 034/001 | Batch 003/137 | Loss: 35.0758\n",
      "Epoch: 034/001 | Batch 004/137 | Loss: 40.2815\n",
      "Epoch: 034/001 | Batch 005/137 | Loss: 37.3361\n",
      "Epoch: 034/001 | Batch 006/137 | Loss: 38.4663\n",
      "Epoch: 034/001 | Batch 007/137 | Loss: 40.1608\n",
      "Epoch: 034/001 | Batch 008/137 | Loss: 35.6334\n",
      "Epoch: 034/001 | Batch 009/137 | Loss: 39.0208\n",
      "Epoch: 034/001 | Batch 010/137 | Loss: 36.3919\n",
      "Epoch: 034/001 | Batch 011/137 | Loss: 36.9533\n",
      "Epoch: 034/001 | Batch 012/137 | Loss: 34.7187\n",
      "Epoch: 034/001 | Batch 013/137 | Loss: 38.3909\n",
      "Epoch: 034/001 | Batch 014/137 | Loss: 37.1952\n",
      "Epoch: 034/001 | Batch 015/137 | Loss: 35.1484\n",
      "Epoch: 034/001 | Batch 016/137 | Loss: 38.6331\n",
      "Epoch: 034/001 | Batch 017/137 | Loss: 36.8178\n",
      "Epoch: 034/001 | Batch 018/137 | Loss: 37.9657\n",
      "Epoch: 034/001 | Batch 019/137 | Loss: 37.9124\n",
      "Epoch: 034/001 | Batch 020/137 | Loss: 35.0675\n",
      "Epoch: 034/001 | Batch 021/137 | Loss: 36.1139\n",
      "Epoch: 034/001 | Batch 022/137 | Loss: 36.7358\n",
      "Epoch: 034/001 | Batch 023/137 | Loss: 18.0475\n",
      "Epoch: 035/001 | Batch 000/137 | Loss: 37.7334\n",
      "Epoch: 035/001 | Batch 001/137 | Loss: 38.6184\n",
      "Epoch: 035/001 | Batch 002/137 | Loss: 38.3880\n",
      "Epoch: 035/001 | Batch 003/137 | Loss: 35.0253\n",
      "Epoch: 035/001 | Batch 004/137 | Loss: 40.2386\n",
      "Epoch: 035/001 | Batch 005/137 | Loss: 37.3501\n",
      "Epoch: 035/001 | Batch 006/137 | Loss: 38.3913\n",
      "Epoch: 035/001 | Batch 007/137 | Loss: 40.1263\n",
      "Epoch: 035/001 | Batch 008/137 | Loss: 35.6163\n",
      "Epoch: 035/001 | Batch 009/137 | Loss: 38.9976\n",
      "Epoch: 035/001 | Batch 010/137 | Loss: 36.3531\n",
      "Epoch: 035/001 | Batch 011/137 | Loss: 36.9940\n",
      "Epoch: 035/001 | Batch 012/137 | Loss: 34.7038\n",
      "Epoch: 035/001 | Batch 013/137 | Loss: 38.3628\n",
      "Epoch: 035/001 | Batch 014/137 | Loss: 37.1895\n",
      "Epoch: 035/001 | Batch 015/137 | Loss: 35.1583\n",
      "Epoch: 035/001 | Batch 016/137 | Loss: 38.5904\n",
      "Epoch: 035/001 | Batch 017/137 | Loss: 36.7622\n",
      "Epoch: 035/001 | Batch 018/137 | Loss: 37.8763\n",
      "Epoch: 035/001 | Batch 019/137 | Loss: 37.8919\n",
      "Epoch: 035/001 | Batch 020/137 | Loss: 35.0650\n",
      "Epoch: 035/001 | Batch 021/137 | Loss: 36.0499\n",
      "Epoch: 035/001 | Batch 022/137 | Loss: 36.7534\n",
      "Epoch: 035/001 | Batch 023/137 | Loss: 17.8595\n",
      "Epoch: 036/001 | Batch 000/137 | Loss: 37.7076\n",
      "Epoch: 036/001 | Batch 001/137 | Loss: 38.5721\n",
      "Epoch: 036/001 | Batch 002/137 | Loss: 38.3868\n",
      "Epoch: 036/001 | Batch 003/137 | Loss: 34.9929\n",
      "Epoch: 036/001 | Batch 004/137 | Loss: 40.2016\n",
      "Epoch: 036/001 | Batch 005/137 | Loss: 37.2545\n",
      "Epoch: 036/001 | Batch 006/137 | Loss: 38.3873\n",
      "Epoch: 036/001 | Batch 007/137 | Loss: 40.1449\n",
      "Epoch: 036/001 | Batch 008/137 | Loss: 35.5883\n",
      "Epoch: 036/001 | Batch 009/137 | Loss: 39.0572\n",
      "Epoch: 036/001 | Batch 010/137 | Loss: 36.2911\n",
      "Epoch: 036/001 | Batch 011/137 | Loss: 36.9173\n",
      "Epoch: 036/001 | Batch 012/137 | Loss: 34.7221\n",
      "Epoch: 036/001 | Batch 013/137 | Loss: 38.3907\n",
      "Epoch: 036/001 | Batch 014/137 | Loss: 37.2594\n",
      "Epoch: 036/001 | Batch 015/137 | Loss: 35.1616\n",
      "Epoch: 036/001 | Batch 016/137 | Loss: 38.5332\n",
      "Epoch: 036/001 | Batch 017/137 | Loss: 36.8305\n",
      "Epoch: 036/001 | Batch 018/137 | Loss: 37.9093\n",
      "Epoch: 036/001 | Batch 019/137 | Loss: 37.8347\n",
      "Epoch: 036/001 | Batch 020/137 | Loss: 35.0380\n",
      "Epoch: 036/001 | Batch 021/137 | Loss: 36.0062\n",
      "Epoch: 036/001 | Batch 022/137 | Loss: 36.7488\n",
      "Epoch: 036/001 | Batch 023/137 | Loss: 17.6837\n",
      "Epoch: 037/001 | Batch 000/137 | Loss: 37.7694\n",
      "Epoch: 037/001 | Batch 001/137 | Loss: 38.5745\n",
      "Epoch: 037/001 | Batch 002/137 | Loss: 38.4069\n",
      "Epoch: 037/001 | Batch 003/137 | Loss: 35.0152\n",
      "Epoch: 037/001 | Batch 004/137 | Loss: 40.2607\n",
      "Epoch: 037/001 | Batch 005/137 | Loss: 37.2417\n",
      "Epoch: 037/001 | Batch 006/137 | Loss: 38.3702\n",
      "Epoch: 037/001 | Batch 007/137 | Loss: 40.1133\n",
      "Epoch: 037/001 | Batch 008/137 | Loss: 35.5624\n",
      "Epoch: 037/001 | Batch 009/137 | Loss: 38.9382\n",
      "Epoch: 037/001 | Batch 010/137 | Loss: 36.2901\n",
      "Epoch: 037/001 | Batch 011/137 | Loss: 36.9138\n",
      "Epoch: 037/001 | Batch 012/137 | Loss: 34.7157\n",
      "Epoch: 037/001 | Batch 013/137 | Loss: 38.3740\n",
      "Epoch: 037/001 | Batch 014/137 | Loss: 37.2359\n",
      "Epoch: 037/001 | Batch 015/137 | Loss: 35.1454\n",
      "Epoch: 037/001 | Batch 016/137 | Loss: 38.5192\n",
      "Epoch: 037/001 | Batch 017/137 | Loss: 36.7444\n",
      "Epoch: 037/001 | Batch 018/137 | Loss: 37.9075\n",
      "Epoch: 037/001 | Batch 019/137 | Loss: 37.8083\n",
      "Epoch: 037/001 | Batch 020/137 | Loss: 35.0450\n",
      "Epoch: 037/001 | Batch 021/137 | Loss: 36.0112\n",
      "Epoch: 037/001 | Batch 022/137 | Loss: 36.7248\n",
      "Epoch: 037/001 | Batch 023/137 | Loss: 17.5922\n",
      "Epoch: 038/001 | Batch 000/137 | Loss: 37.6672\n",
      "Epoch: 038/001 | Batch 001/137 | Loss: 38.5552\n",
      "Epoch: 038/001 | Batch 002/137 | Loss: 38.4337\n",
      "Epoch: 038/001 | Batch 003/137 | Loss: 34.9424\n",
      "Epoch: 038/001 | Batch 004/137 | Loss: 40.3097\n",
      "Epoch: 038/001 | Batch 005/137 | Loss: 37.3820\n",
      "Epoch: 038/001 | Batch 006/137 | Loss: 38.3439\n",
      "Epoch: 038/001 | Batch 007/137 | Loss: 40.1001\n",
      "Epoch: 038/001 | Batch 008/137 | Loss: 35.5604\n",
      "Epoch: 038/001 | Batch 009/137 | Loss: 38.9646\n",
      "Epoch: 038/001 | Batch 010/137 | Loss: 36.3299\n",
      "Epoch: 038/001 | Batch 011/137 | Loss: 36.9631\n",
      "Epoch: 038/001 | Batch 012/137 | Loss: 34.7046\n",
      "Epoch: 038/001 | Batch 013/137 | Loss: 38.3454\n",
      "Epoch: 038/001 | Batch 014/137 | Loss: 37.2475\n",
      "Epoch: 038/001 | Batch 015/137 | Loss: 35.1677\n",
      "Epoch: 038/001 | Batch 016/137 | Loss: 38.5310\n",
      "Epoch: 038/001 | Batch 017/137 | Loss: 36.7928\n",
      "Epoch: 038/001 | Batch 018/137 | Loss: 37.9482\n",
      "Epoch: 038/001 | Batch 019/137 | Loss: 37.8670\n",
      "Epoch: 038/001 | Batch 020/137 | Loss: 35.0757\n",
      "Epoch: 038/001 | Batch 021/137 | Loss: 35.9712\n",
      "Epoch: 038/001 | Batch 022/137 | Loss: 36.7325\n",
      "Epoch: 038/001 | Batch 023/137 | Loss: 17.5146\n",
      "Epoch: 039/001 | Batch 000/137 | Loss: 37.6522\n",
      "Epoch: 039/001 | Batch 001/137 | Loss: 38.5283\n",
      "Epoch: 039/001 | Batch 002/137 | Loss: 38.3835\n",
      "Epoch: 039/001 | Batch 003/137 | Loss: 34.9143\n",
      "Epoch: 039/001 | Batch 004/137 | Loss: 40.2655\n",
      "Epoch: 039/001 | Batch 005/137 | Loss: 37.4261\n",
      "Epoch: 039/001 | Batch 006/137 | Loss: 38.3130\n",
      "Epoch: 039/001 | Batch 007/137 | Loss: 40.0894\n",
      "Epoch: 039/001 | Batch 008/137 | Loss: 35.5340\n",
      "Epoch: 039/001 | Batch 009/137 | Loss: 38.9178\n",
      "Epoch: 039/001 | Batch 010/137 | Loss: 36.3082\n",
      "Epoch: 039/001 | Batch 011/137 | Loss: 36.9127\n",
      "Epoch: 039/001 | Batch 012/137 | Loss: 34.6928\n",
      "Epoch: 039/001 | Batch 013/137 | Loss: 38.3573\n",
      "Epoch: 039/001 | Batch 014/137 | Loss: 37.2284\n",
      "Epoch: 039/001 | Batch 015/137 | Loss: 35.1671\n",
      "Epoch: 039/001 | Batch 016/137 | Loss: 38.5310\n",
      "Epoch: 039/001 | Batch 017/137 | Loss: 36.7499\n",
      "Epoch: 039/001 | Batch 018/137 | Loss: 37.9317\n",
      "Epoch: 039/001 | Batch 019/137 | Loss: 37.8395\n",
      "Epoch: 039/001 | Batch 020/137 | Loss: 35.0785\n",
      "Epoch: 039/001 | Batch 021/137 | Loss: 36.0096\n",
      "Epoch: 039/001 | Batch 022/137 | Loss: 36.7445\n",
      "Epoch: 039/001 | Batch 023/137 | Loss: 17.4187\n",
      "Epoch: 040/001 | Batch 000/137 | Loss: 37.6724\n",
      "Epoch: 040/001 | Batch 001/137 | Loss: 38.5857\n",
      "Epoch: 040/001 | Batch 002/137 | Loss: 38.3782\n",
      "Epoch: 040/001 | Batch 003/137 | Loss: 34.8896\n",
      "Epoch: 040/001 | Batch 004/137 | Loss: 40.2535\n",
      "Epoch: 040/001 | Batch 005/137 | Loss: 37.3575\n",
      "Epoch: 040/001 | Batch 006/137 | Loss: 38.3150\n",
      "Epoch: 040/001 | Batch 007/137 | Loss: 40.1581\n",
      "Epoch: 040/001 | Batch 008/137 | Loss: 35.5555\n",
      "Epoch: 040/001 | Batch 009/137 | Loss: 39.0351\n",
      "Epoch: 040/001 | Batch 010/137 | Loss: 36.2952\n",
      "Epoch: 040/001 | Batch 011/137 | Loss: 36.9012\n",
      "Epoch: 040/001 | Batch 012/137 | Loss: 34.6744\n",
      "Epoch: 040/001 | Batch 013/137 | Loss: 38.3748\n",
      "Epoch: 040/001 | Batch 014/137 | Loss: 37.2075\n",
      "Epoch: 040/001 | Batch 015/137 | Loss: 35.1527\n",
      "Epoch: 040/001 | Batch 016/137 | Loss: 38.5366\n",
      "Epoch: 040/001 | Batch 017/137 | Loss: 36.7287\n",
      "Epoch: 040/001 | Batch 018/137 | Loss: 37.9162\n",
      "Epoch: 040/001 | Batch 019/137 | Loss: 37.8346\n",
      "Epoch: 040/001 | Batch 020/137 | Loss: 35.0482\n",
      "Epoch: 040/001 | Batch 021/137 | Loss: 35.9912\n",
      "Epoch: 040/001 | Batch 022/137 | Loss: 36.7478\n",
      "Epoch: 040/001 | Batch 023/137 | Loss: 17.4239\n",
      "Epoch: 041/001 | Batch 000/137 | Loss: 37.6528\n",
      "Epoch: 041/001 | Batch 001/137 | Loss: 38.5437\n",
      "Epoch: 041/001 | Batch 002/137 | Loss: 38.3737\n",
      "Epoch: 041/001 | Batch 003/137 | Loss: 34.8904\n",
      "Epoch: 041/001 | Batch 004/137 | Loss: 40.1752\n",
      "Epoch: 041/001 | Batch 005/137 | Loss: 37.1991\n",
      "Epoch: 041/001 | Batch 006/137 | Loss: 38.2949\n",
      "Epoch: 041/001 | Batch 007/137 | Loss: 40.1545\n",
      "Epoch: 041/001 | Batch 008/137 | Loss: 35.6018\n",
      "Epoch: 041/001 | Batch 009/137 | Loss: 38.9645\n",
      "Epoch: 041/001 | Batch 010/137 | Loss: 36.2224\n",
      "Epoch: 041/001 | Batch 011/137 | Loss: 36.8464\n",
      "Epoch: 041/001 | Batch 012/137 | Loss: 34.5755\n",
      "Epoch: 041/001 | Batch 013/137 | Loss: 38.3246\n",
      "Epoch: 041/001 | Batch 014/137 | Loss: 37.1674\n",
      "Epoch: 041/001 | Batch 015/137 | Loss: 35.1296\n",
      "Epoch: 041/001 | Batch 016/137 | Loss: 38.5233\n",
      "Epoch: 041/001 | Batch 017/137 | Loss: 36.7501\n",
      "Epoch: 041/001 | Batch 018/137 | Loss: 37.8707\n",
      "Epoch: 041/001 | Batch 019/137 | Loss: 37.7980\n",
      "Epoch: 041/001 | Batch 020/137 | Loss: 35.0207\n",
      "Epoch: 041/001 | Batch 021/137 | Loss: 35.9772\n",
      "Epoch: 041/001 | Batch 022/137 | Loss: 36.7008\n",
      "Epoch: 041/001 | Batch 023/137 | Loss: 17.3027\n",
      "Epoch: 042/001 | Batch 000/137 | Loss: 37.6307\n",
      "Epoch: 042/001 | Batch 001/137 | Loss: 38.5791\n",
      "Epoch: 042/001 | Batch 002/137 | Loss: 38.3277\n",
      "Epoch: 042/001 | Batch 003/137 | Loss: 34.8516\n",
      "Epoch: 042/001 | Batch 004/137 | Loss: 40.1941\n",
      "Epoch: 042/001 | Batch 005/137 | Loss: 37.2848\n",
      "Epoch: 042/001 | Batch 006/137 | Loss: 38.2805\n",
      "Epoch: 042/001 | Batch 007/137 | Loss: 40.1075\n",
      "Epoch: 042/001 | Batch 008/137 | Loss: 35.5232\n",
      "Epoch: 042/001 | Batch 009/137 | Loss: 38.8906\n",
      "Epoch: 042/001 | Batch 010/137 | Loss: 36.2160\n",
      "Epoch: 042/001 | Batch 011/137 | Loss: 36.8466\n",
      "Epoch: 042/001 | Batch 012/137 | Loss: 34.5828\n",
      "Epoch: 042/001 | Batch 013/137 | Loss: 38.3459\n",
      "Epoch: 042/001 | Batch 014/137 | Loss: 37.2107\n",
      "Epoch: 042/001 | Batch 015/137 | Loss: 35.1056\n",
      "Epoch: 042/001 | Batch 016/137 | Loss: 38.5279\n",
      "Epoch: 042/001 | Batch 017/137 | Loss: 36.7353\n",
      "Epoch: 042/001 | Batch 018/137 | Loss: 37.9169\n",
      "Epoch: 042/001 | Batch 019/137 | Loss: 37.8331\n",
      "Epoch: 042/001 | Batch 020/137 | Loss: 34.9887\n",
      "Epoch: 042/001 | Batch 021/137 | Loss: 35.9598\n",
      "Epoch: 042/001 | Batch 022/137 | Loss: 36.6804\n",
      "Epoch: 042/001 | Batch 023/137 | Loss: 17.4609\n",
      "Epoch: 043/001 | Batch 000/137 | Loss: 37.6339\n",
      "Epoch: 043/001 | Batch 001/137 | Loss: 38.5018\n",
      "Epoch: 043/001 | Batch 002/137 | Loss: 38.2990\n",
      "Epoch: 043/001 | Batch 003/137 | Loss: 34.8585\n",
      "Epoch: 043/001 | Batch 004/137 | Loss: 40.1327\n",
      "Epoch: 043/001 | Batch 005/137 | Loss: 37.3347\n",
      "Epoch: 043/001 | Batch 006/137 | Loss: 38.3382\n",
      "Epoch: 043/001 | Batch 007/137 | Loss: 40.1526\n",
      "Epoch: 043/001 | Batch 008/137 | Loss: 35.5739\n",
      "Epoch: 043/001 | Batch 009/137 | Loss: 38.9864\n",
      "Epoch: 043/001 | Batch 010/137 | Loss: 36.1507\n",
      "Epoch: 043/001 | Batch 011/137 | Loss: 36.8453\n",
      "Epoch: 043/001 | Batch 012/137 | Loss: 34.6114\n",
      "Epoch: 043/001 | Batch 013/137 | Loss: 38.3545\n",
      "Epoch: 043/001 | Batch 014/137 | Loss: 37.2394\n",
      "Epoch: 043/001 | Batch 015/137 | Loss: 35.1342\n",
      "Epoch: 043/001 | Batch 016/137 | Loss: 38.5159\n",
      "Epoch: 043/001 | Batch 017/137 | Loss: 36.7397\n",
      "Epoch: 043/001 | Batch 018/137 | Loss: 37.9239\n",
      "Epoch: 043/001 | Batch 019/137 | Loss: 37.8381\n",
      "Epoch: 043/001 | Batch 020/137 | Loss: 35.0187\n",
      "Epoch: 043/001 | Batch 021/137 | Loss: 35.9759\n",
      "Epoch: 043/001 | Batch 022/137 | Loss: 36.7037\n",
      "Epoch: 043/001 | Batch 023/137 | Loss: 17.2713\n",
      "Epoch: 044/001 | Batch 000/137 | Loss: 37.6662\n",
      "Epoch: 044/001 | Batch 001/137 | Loss: 38.5690\n",
      "Epoch: 044/001 | Batch 002/137 | Loss: 38.3591\n",
      "Epoch: 044/001 | Batch 003/137 | Loss: 34.8811\n",
      "Epoch: 044/001 | Batch 004/137 | Loss: 40.1710\n",
      "Epoch: 044/001 | Batch 005/137 | Loss: 37.2929\n",
      "Epoch: 044/001 | Batch 006/137 | Loss: 38.2940\n",
      "Epoch: 044/001 | Batch 007/137 | Loss: 40.1060\n",
      "Epoch: 044/001 | Batch 008/137 | Loss: 35.6394\n",
      "Epoch: 044/001 | Batch 009/137 | Loss: 38.9216\n",
      "Epoch: 044/001 | Batch 010/137 | Loss: 36.2849\n",
      "Epoch: 044/001 | Batch 011/137 | Loss: 36.8856\n",
      "Epoch: 044/001 | Batch 012/137 | Loss: 34.6994\n",
      "Epoch: 044/001 | Batch 013/137 | Loss: 38.3915\n",
      "Epoch: 044/001 | Batch 014/137 | Loss: 37.2216\n",
      "Epoch: 044/001 | Batch 015/137 | Loss: 35.1646\n",
      "Epoch: 044/001 | Batch 016/137 | Loss: 38.5226\n",
      "Epoch: 044/001 | Batch 017/137 | Loss: 36.7370\n",
      "Epoch: 044/001 | Batch 018/137 | Loss: 37.9293\n",
      "Epoch: 044/001 | Batch 019/137 | Loss: 37.8265\n",
      "Epoch: 044/001 | Batch 020/137 | Loss: 35.0705\n",
      "Epoch: 044/001 | Batch 021/137 | Loss: 35.9890\n",
      "Epoch: 044/001 | Batch 022/137 | Loss: 36.7382\n",
      "Epoch: 044/001 | Batch 023/137 | Loss: 17.7554\n",
      "Epoch: 045/001 | Batch 000/137 | Loss: 37.6963\n",
      "Epoch: 045/001 | Batch 001/137 | Loss: 38.5368\n",
      "Epoch: 045/001 | Batch 002/137 | Loss: 38.4153\n",
      "Epoch: 045/001 | Batch 003/137 | Loss: 34.9534\n",
      "Epoch: 045/001 | Batch 004/137 | Loss: 40.2016\n",
      "Epoch: 045/001 | Batch 005/137 | Loss: 37.2775\n",
      "Epoch: 045/001 | Batch 006/137 | Loss: 38.3500\n",
      "Epoch: 045/001 | Batch 007/137 | Loss: 40.1260\n",
      "Epoch: 045/001 | Batch 008/137 | Loss: 35.5415\n",
      "Epoch: 045/001 | Batch 009/137 | Loss: 38.9417\n",
      "Epoch: 045/001 | Batch 010/137 | Loss: 36.2912\n",
      "Epoch: 045/001 | Batch 011/137 | Loss: 36.8416\n",
      "Epoch: 045/001 | Batch 012/137 | Loss: 34.6895\n",
      "Epoch: 045/001 | Batch 013/137 | Loss: 38.4047\n",
      "Epoch: 045/001 | Batch 014/137 | Loss: 37.2348\n",
      "Epoch: 045/001 | Batch 015/137 | Loss: 35.1108\n",
      "Epoch: 045/001 | Batch 016/137 | Loss: 38.5536\n",
      "Epoch: 045/001 | Batch 017/137 | Loss: 36.7548\n",
      "Epoch: 045/001 | Batch 018/137 | Loss: 37.9138\n",
      "Epoch: 045/001 | Batch 019/137 | Loss: 37.8520\n",
      "Epoch: 045/001 | Batch 020/137 | Loss: 35.0872\n",
      "Epoch: 045/001 | Batch 021/137 | Loss: 36.0099\n",
      "Epoch: 045/001 | Batch 022/137 | Loss: 36.7267\n",
      "Epoch: 045/001 | Batch 023/137 | Loss: 17.4329\n",
      "Epoch: 046/001 | Batch 000/137 | Loss: 37.6885\n",
      "Epoch: 046/001 | Batch 001/137 | Loss: 38.5534\n",
      "Epoch: 046/001 | Batch 002/137 | Loss: 38.3825\n",
      "Epoch: 046/001 | Batch 003/137 | Loss: 34.9469\n",
      "Epoch: 046/001 | Batch 004/137 | Loss: 40.2590\n",
      "Epoch: 046/001 | Batch 005/137 | Loss: 37.4968\n",
      "Epoch: 046/001 | Batch 006/137 | Loss: 38.3422\n",
      "Epoch: 046/001 | Batch 007/137 | Loss: 40.1395\n",
      "Epoch: 046/001 | Batch 008/137 | Loss: 35.5838\n",
      "Epoch: 046/001 | Batch 009/137 | Loss: 38.9996\n",
      "Epoch: 046/001 | Batch 010/137 | Loss: 36.3487\n",
      "Epoch: 046/001 | Batch 011/137 | Loss: 36.9829\n",
      "Epoch: 046/001 | Batch 012/137 | Loss: 34.6610\n",
      "Epoch: 046/001 | Batch 013/137 | Loss: 38.3832\n",
      "Epoch: 046/001 | Batch 014/137 | Loss: 37.2387\n",
      "Epoch: 046/001 | Batch 015/137 | Loss: 35.2113\n",
      "Epoch: 046/001 | Batch 016/137 | Loss: 38.6446\n",
      "Epoch: 046/001 | Batch 017/137 | Loss: 36.7533\n",
      "Epoch: 046/001 | Batch 018/137 | Loss: 37.9683\n",
      "Epoch: 046/001 | Batch 019/137 | Loss: 37.8974\n",
      "Epoch: 046/001 | Batch 020/137 | Loss: 35.0794\n",
      "Epoch: 046/001 | Batch 021/137 | Loss: 36.0611\n",
      "Epoch: 046/001 | Batch 022/137 | Loss: 36.7490\n",
      "Epoch: 046/001 | Batch 023/137 | Loss: 17.3019\n",
      "Epoch: 047/001 | Batch 000/137 | Loss: 37.7446\n",
      "Epoch: 047/001 | Batch 001/137 | Loss: 38.5738\n",
      "Epoch: 047/001 | Batch 002/137 | Loss: 38.3428\n",
      "Epoch: 047/001 | Batch 003/137 | Loss: 34.9409\n",
      "Epoch: 047/001 | Batch 004/137 | Loss: 40.2632\n",
      "Epoch: 047/001 | Batch 005/137 | Loss: 37.3474\n",
      "Epoch: 047/001 | Batch 006/137 | Loss: 38.4310\n",
      "Epoch: 047/001 | Batch 007/137 | Loss: 40.1120\n",
      "Epoch: 047/001 | Batch 008/137 | Loss: 35.6041\n",
      "Epoch: 047/001 | Batch 009/137 | Loss: 39.1115\n",
      "Epoch: 047/001 | Batch 010/137 | Loss: 36.3220\n",
      "Epoch: 047/001 | Batch 011/137 | Loss: 36.8633\n",
      "Epoch: 047/001 | Batch 012/137 | Loss: 34.6604\n",
      "Epoch: 047/001 | Batch 013/137 | Loss: 38.3912\n",
      "Epoch: 047/001 | Batch 014/137 | Loss: 37.2806\n",
      "Epoch: 047/001 | Batch 015/137 | Loss: 35.1128\n",
      "Epoch: 047/001 | Batch 016/137 | Loss: 38.5708\n",
      "Epoch: 047/001 | Batch 017/137 | Loss: 36.7401\n",
      "Epoch: 047/001 | Batch 018/137 | Loss: 37.9559\n",
      "Epoch: 047/001 | Batch 019/137 | Loss: 37.8898\n",
      "Epoch: 047/001 | Batch 020/137 | Loss: 35.1055\n",
      "Epoch: 047/001 | Batch 021/137 | Loss: 36.0008\n",
      "Epoch: 047/001 | Batch 022/137 | Loss: 36.7933\n",
      "Epoch: 047/001 | Batch 023/137 | Loss: 17.1444\n",
      "Epoch: 048/001 | Batch 000/137 | Loss: 37.6938\n",
      "Epoch: 048/001 | Batch 001/137 | Loss: 38.5794\n",
      "Epoch: 048/001 | Batch 002/137 | Loss: 38.3390\n",
      "Epoch: 048/001 | Batch 003/137 | Loss: 34.9297\n",
      "Epoch: 048/001 | Batch 004/137 | Loss: 40.1769\n",
      "Epoch: 048/001 | Batch 005/137 | Loss: 37.3808\n",
      "Epoch: 048/001 | Batch 006/137 | Loss: 38.4958\n",
      "Epoch: 048/001 | Batch 007/137 | Loss: 40.1422\n",
      "Epoch: 048/001 | Batch 008/137 | Loss: 35.5532\n",
      "Epoch: 048/001 | Batch 009/137 | Loss: 39.0394\n",
      "Epoch: 048/001 | Batch 010/137 | Loss: 36.2706\n",
      "Epoch: 048/001 | Batch 011/137 | Loss: 36.8240\n",
      "Epoch: 048/001 | Batch 012/137 | Loss: 34.6474\n",
      "Epoch: 048/001 | Batch 013/137 | Loss: 38.3982\n",
      "Epoch: 048/001 | Batch 014/137 | Loss: 37.3063\n",
      "Epoch: 048/001 | Batch 015/137 | Loss: 35.1443\n",
      "Epoch: 048/001 | Batch 016/137 | Loss: 38.5839\n",
      "Epoch: 048/001 | Batch 017/137 | Loss: 36.7634\n",
      "Epoch: 048/001 | Batch 018/137 | Loss: 37.9885\n",
      "Epoch: 048/001 | Batch 019/137 | Loss: 37.8823\n",
      "Epoch: 048/001 | Batch 020/137 | Loss: 35.0309\n",
      "Epoch: 048/001 | Batch 021/137 | Loss: 35.9936\n",
      "Epoch: 048/001 | Batch 022/137 | Loss: 36.7609\n",
      "Epoch: 048/001 | Batch 023/137 | Loss: 17.1575\n",
      "Epoch: 049/001 | Batch 000/137 | Loss: 37.6580\n",
      "Epoch: 049/001 | Batch 001/137 | Loss: 38.5588\n",
      "Epoch: 049/001 | Batch 002/137 | Loss: 38.3528\n",
      "Epoch: 049/001 | Batch 003/137 | Loss: 34.8886\n",
      "Epoch: 049/001 | Batch 004/137 | Loss: 40.1913\n",
      "Epoch: 049/001 | Batch 005/137 | Loss: 37.3091\n",
      "Epoch: 049/001 | Batch 006/137 | Loss: 38.4208\n",
      "Epoch: 049/001 | Batch 007/137 | Loss: 40.1099\n",
      "Epoch: 049/001 | Batch 008/137 | Loss: 35.5242\n",
      "Epoch: 049/001 | Batch 009/137 | Loss: 38.9831\n",
      "Epoch: 049/001 | Batch 010/137 | Loss: 36.2113\n",
      "Epoch: 049/001 | Batch 011/137 | Loss: 36.8379\n",
      "Epoch: 049/001 | Batch 012/137 | Loss: 34.5723\n",
      "Epoch: 049/001 | Batch 013/137 | Loss: 38.2969\n",
      "Epoch: 049/001 | Batch 014/137 | Loss: 37.1853\n",
      "Epoch: 049/001 | Batch 015/137 | Loss: 35.1379\n",
      "Epoch: 049/001 | Batch 016/137 | Loss: 38.5274\n",
      "Epoch: 049/001 | Batch 017/137 | Loss: 36.7166\n",
      "Epoch: 049/001 | Batch 018/137 | Loss: 37.9225\n",
      "Epoch: 049/001 | Batch 019/137 | Loss: 37.8690\n",
      "Epoch: 049/001 | Batch 020/137 | Loss: 35.0069\n",
      "Epoch: 049/001 | Batch 021/137 | Loss: 36.0183\n",
      "Epoch: 049/001 | Batch 022/137 | Loss: 36.7034\n",
      "Epoch: 049/001 | Batch 023/137 | Loss: 16.9812\n",
      "Epoch: 050/001 | Batch 000/137 | Loss: 37.6695\n",
      "Epoch: 050/001 | Batch 001/137 | Loss: 38.5790\n",
      "Epoch: 050/001 | Batch 002/137 | Loss: 38.3178\n",
      "Epoch: 050/001 | Batch 003/137 | Loss: 34.9215\n",
      "Epoch: 050/001 | Batch 004/137 | Loss: 40.1432\n",
      "Epoch: 050/001 | Batch 005/137 | Loss: 37.1823\n",
      "Epoch: 050/001 | Batch 006/137 | Loss: 38.3512\n",
      "Epoch: 050/001 | Batch 007/137 | Loss: 40.0685\n",
      "Epoch: 050/001 | Batch 008/137 | Loss: 35.4866\n",
      "Epoch: 050/001 | Batch 009/137 | Loss: 38.9417\n",
      "Epoch: 050/001 | Batch 010/137 | Loss: 36.2017\n",
      "Epoch: 050/001 | Batch 011/137 | Loss: 36.8882\n",
      "Epoch: 050/001 | Batch 012/137 | Loss: 34.5928\n",
      "Epoch: 050/001 | Batch 013/137 | Loss: 38.3777\n",
      "Epoch: 050/001 | Batch 014/137 | Loss: 37.1528\n",
      "Epoch: 050/001 | Batch 015/137 | Loss: 35.0905\n",
      "Epoch: 050/001 | Batch 016/137 | Loss: 38.5289\n",
      "Epoch: 050/001 | Batch 017/137 | Loss: 36.6932\n",
      "Epoch: 050/001 | Batch 018/137 | Loss: 37.9127\n",
      "Epoch: 050/001 | Batch 019/137 | Loss: 37.8557\n",
      "Epoch: 050/001 | Batch 020/137 | Loss: 35.0001\n",
      "Epoch: 050/001 | Batch 021/137 | Loss: 36.0179\n",
      "Epoch: 050/001 | Batch 022/137 | Loss: 36.6879\n",
      "Epoch: 050/001 | Batch 023/137 | Loss: 16.8566\n",
      "Total Training Time: 0.50 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "NUM_EPOCHS=50\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    count=0\n",
    "    hidden_state=None\n",
    "    for i in batch :\n",
    "        input,output,sequence=i.get(\"input\"),i.get(\"output\"),i.get(\"sequence\")\n",
    "        if hidden_state is not None:\n",
    "            model.load_hidden_state(hidden_state)\n",
    "        logits = model(input,sequence)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, output)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        hidden_state=model.save_hidden_state()\n",
    "        # model.reset_hidden_state()\n",
    "\n",
    "        \n",
    "        print (f'Epoch: {epoch+1:03d}/{co.NUM_EPOCHS:03d} | '\n",
    "                   f'Batch {count:03d}/{len(batch_date):03d} | '\n",
    "                   f'Loss: {loss:.4f}')\n",
    "        count+=1\n",
    "    model.reset_hidden_state()\n",
    "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
